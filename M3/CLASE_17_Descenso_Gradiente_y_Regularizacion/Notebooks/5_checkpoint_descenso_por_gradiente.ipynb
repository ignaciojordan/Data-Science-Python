{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "    \n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://www.digitalhouse.com/ar/logo-DH.png\" width=\"400\" height=\"200\" align='right'>](http://digitalhouse.com.ar/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXCJlrSOyhgz"
   },
   "source": [
    "# CHEKPOINT - Descenso del Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMyNQNI6yhg3"
   },
   "source": [
    "En este checkpoint veremos cómo implementar el método del descenso del gradiente en Python para una regresión lineal simple. Este método de optimización es uno de los más utilizados en machine learning y comprender su funcionamiento es clave para entender cómo se llevan a cabo el entrenamiento de los modelos.\n",
    "\n",
    "El Algoritmo de Descenso del gradiente, que es utilizado por varios modelos para minimizar la **funcion de coste J(θ)**, y como consecuencia encontrar los parámetros que minimiza el error de predicción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YU2jeNdxyhg4"
   },
   "source": [
    "## Funcionamiento del Algoritmo de Descenso del Gradiente\n",
    "\n",
    "Como indicamos, este algoritmo permite permite optimizar los modelos predictivos buscando minimizar la funcion de coste J(θ). Esto lo hace ajustando gradual y sucesivamente los parametros del modelo de forma que con cada nuevo ajuste logra reducir el valor de J(θ) hasta finalmente alcanzar el mínimo más próximo. Este puede ser tanto un mínimo local como global, ya que este algoritmo no los puede diferenciar.  \n",
    "\n",
    "Para identificar el mínimo de la función el método del descenso del gradiente calcula la derivada parcial de la funcion de coste J(θ) respecto a cada parámetro del modelo en el punto de evaluación. La derivada indica el valor y sentido en que se encuentra el minimo mas cercano. \n",
    "El resultado de la derivada se le resta a cada uno de los parámetros multiplicado por la velocidad de aprendizaje (α). La velocidad de aprendizaje generalmente tiene un valor entre 0 y 1 e indica lo rápido que converge el algoritmo. Es importante notar que es necesario seleccionar un valor adecuado. Un valor demasiado bajo puede provocar que el algoritmo sea muy lento. Por otro lado, un valor lo demasiado alto podría saltarse el mínimo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, los pasos que realiza el algoritmo de descenso del gradiente son los siguiente:\n",
    "\n",
    "1) Inicializar los parámetros θ a un valor de inicio  \n",
    "2) Indicar la velocidad de aprendizaje del algoritmo (α)  \n",
    "3) Obtener la derivada de J en el punto θ  \n",
    "4) Sustraer la derivada por la velocidad de aprendizaje al valor actual del parámetro  \n",
    "5) Actualizar el valor de θ con el nuevo valor  \n",
    "6) Comprobar el cambio en la actualización de los parámetros es inferior a un fijado previamente (llamada criterio de parada).  \n",
    "7) En caso afirmativo finalizar la ejecución, en caso contrario volver al punto 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si usamos como ejemplo un modelo de regresion simple, con solo una variable predictiva y una varaible target; tendremos que la ecuacion de la recta se puede escribir como:\n",
    "\n",
    "$$\\widehat{y} = b_{0} + b_{1}*x$$  \n",
    "\n",
    "la funcion de coste que utilizaremos es:\n",
    "\n",
    "$$J_{\\Theta}= \\frac{1}{2N}\\sum_{1}^{N}(\\widehat{y}^{(i)}-y^{(i)})^{2}$$  \n",
    "\n",
    "$$J_{\\Theta}= \\frac{1}{2N}\\sum_{1}^{N}((b_{0} + b_{1}*x^{(i)})-y^{(i)})^{2}$$  \n",
    "\n",
    "y se puede demostrar que sus derivadas parciales son:\n",
    "\n",
    "$$\\frac{\\partial }{\\partial b_{0}} J_{(b0,b1)}= \\frac{1}{N}\\sum_{1}^{N}((b_{0} + b_{1}*x^{(i)})-y^{(i)})$$  \n",
    "\n",
    "$$\\frac{\\partial }{\\partial b_{1}} J_{(b0,b1)}= \\frac{1}{N}\\sum_{1}^{N}((b_{0} + b_{1}*x^{(i)})-y^{(i)})*x^{(i)}$$  \n",
    "\n",
    "\n",
    "A continuación desarrollaremos un algoritmo de descenso de gradiente en python, siguiendo los los pasos mencionados anteriormente y aplicando las fórmulas recien enunciadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 880
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 855,
     "status": "error",
     "timestamp": 1546527663689,
     "user": {
      "displayName": "Martín Ríos",
      "photoUrl": "",
      "userId": "02594387867327172413"
     },
     "user_tz": 180
    },
    "id": "3Ad7aOLNyhg6",
    "outputId": "b34ab32d-d009-40f8-f9d3-3ee439dd438a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creación de un conjunto de datos para entrenamiento\n",
    "X = np.linspace(-2, 2, 101)\n",
    "y = 3 + 2 * X + np.random.randn(*X.shape) * 0.33\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, alpha=0.5)\n",
    "plt.title('plot')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la funcion de Descenso del gradiente.\n",
    "\n",
    "\n",
    "def descenso_del_gradiente(X,y,num_steps = 100,learningRate = 0.10,criteria = 1e-5):\n",
    "    \"\"\"\n",
    "    Toma como parametros la variable predicora X y la variable target.\n",
    "    Se establece como parametros opcionales:\n",
    "    num_steps: que define la cantidad máxima de saltos como un criterio de salida\n",
    "    learning_rate: define la velocidad de aprendizaje\n",
    "    criteria: define un segundo criterio de salida cuando el error es inferior a ese valor.\n",
    "    \"\"\"\n",
    "    \n",
    "    b_0 = 1\n",
    "    b_1 = 1\n",
    "    \n",
    "    # en esta variable vamos a almacenar el MSE de la corrida anterior para luego calcular \n",
    "    # cuanto se redujo el error en un salto de gradiente.\n",
    "    mse_previo = 0\n",
    "    \n",
    "    # Proceso iterativo como máximo realiza num_steps loops.\n",
    "    for step in range(0, num_steps):\n",
    "        \n",
    "        #luego de inicializar los parametros calulamos el MSE actual para esos parametros\n",
    "        mse_actual = metrics.mean_squared_error(y,  b_0 + b_1*X)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.scatter(X, y, alpha=0.5)\n",
    "        plt.plot(X, b_0 + b_1*X)\n",
    "        plt.title('step: ' + str(step+1))\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.text(3, 6, \"b0  = \" + str(b_0))\n",
    "        plt.text(3, 5, \"b1  = \" + str(b_1))\n",
    "        plt.text(3, 4, \"MSE = \" + str(mse_actual))\n",
    "        if step > 0:\n",
    "            plt.text(3, 3, \"reduccion del MSE = \" + str(round(100*(mse_previo-mse_actual)/mse_previo,4))+\"%\")\n",
    "        plt.show()\n",
    "                  \n",
    "        \n",
    "                     \n",
    "        b_0_gradient = 0\n",
    "        b_1_gradient = 0\n",
    "        N = len(X)\n",
    "        \n",
    "        for i in range(0, len(X)):\n",
    "            b_0_gradient -= (2.0/N) * (y[i] - (b_0 + b_1 * X[i]))\n",
    "            b_1_gradient -= (2.0/N) * (y[i] - (b_0 + b_1 * X[i])) * X[i]\n",
    "\n",
    "        b_0 = b_0 - (learningRate * b_0_gradient)\n",
    "        b_1 = b_1 - (learningRate * b_1_gradient)\n",
    "        \n",
    "        # almaceno el valor de MSE para comparar en el próximo salto\n",
    "        mse_previo = mse_actual\n",
    "                \n",
    "        #verifico criterio de parada \n",
    "        if max(abs(learningRate * b_0_gradient), abs(learningRate * b_1_gradient)) < criteria:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descenso_del_gradiente(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Indicar en qué lineas de código se inicializa los parámetros b0 y b1.  \n",
    "Explicar brevemente por qué se se debe inicializar estos parámetros.  \n",
    "Se pueden inicializar como b0 = 0 y b1 = 0?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Indicar donde se define la velocidad de aprendizaje del algoritmo (α) y cual es su valor por defecto \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Qué define el parámetro de la funcion criteria?\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) En que lineas del código se calculan los valores de las derivadas parciales?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) En que lineas del código se actualiza el valor de b0 y b1 ?  \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Qué observación pueden ver respecto de la velocidad de aprendizaje del modelo?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Workshop_FB_Regresion_Lineal.ipynb",
   "provenance": [
    {
     "file_id": "1tLnxwf-lAA3so9YuHSq1nl6xxmW9kElL",
     "timestamp": 1544685889886
    },
    {
     "file_id": "1kx27Zo-Ir1nMRVUs6Si_Lz9bKhlP8gy4",
     "timestamp": 1530850462629
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
