{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "    \n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"https://www.digitalhouse.com/ar/logo-DH.png\" width=\"400\" height=\"200\" align='right'>](http://digitalhouse.com.ar/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "855vCylCRde_"
   },
   "source": [
    "# Regularización\n",
    "\n",
    "Hasta ahora, cuando ajustamos modelos lineales, seleccionamos el modelo que minimizaba el error cuadrático.\n",
    "Para un modelo de la forma\n",
    "$$y_i = f(x_i) + e_i$$\n",
    "minimizamos la suma\n",
    "$$\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}$$\n",
    "Este es un ejemplo de __funcion de costo__: una función que mide el \"costo\" de los errores de las predicciones de un modelo. Para aplicar la técnica de regularización, modificamos la función de costo, agregando un término que penaliza los modelos por su complejidad. Por ejemplo, podríamos tener una nueva función de costo de la forma:\n",
    "$$\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2 + \\alpha \\theta_i}$$\n",
    "donde el vector $\\theta$ corresponde a los parámetros de nuestro modelo y $\\alpha$ es un hiper parámetro que controla cuán fuerte es la penalización. Un mayor valor de $\\alpha$ significa una mayor penalización, ya que aumenta el costo, que es lo que buscamos minimizar.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Resumen</b> \n",
    "     Este tipo de métodos trata de \"regular\" la complejidad del modelo. Agregar este término extra evita que el modelo se torne mas complejo mitigando el sobre-ajuste. ¿Cómo logra esto?\n",
    "    Modulando o \"achicando\" los coeficientes como vimos en la teoria. \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Veamos un ejemplo clásico de  ajustar un polinomio a un set de datos pequeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6n5ZHryRdfV"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "plt.rcParams['figure.figsize'] = 6,6\n",
    "\n",
    "random.seed(5)\n",
    "\n",
    "# Generamos un dataset de ejemplo\n",
    "def generate_data():\n",
    "    xs = np.arange(-3, 3, 1)\n",
    "    data = [(2 * x - 3 * random.random(), (x - 3 * random.random()) * (x + random.random())) for x in xs]\n",
    "    data.sort()\n",
    "    xs = [x for (x, y) in data]\n",
    "    ys = [y for (x, y) in data]\n",
    "    \n",
    "    return xs, ys\n",
    "\n",
    "xs, ys = generate_data()\n",
    "plt.scatter(xs, ys)\n",
    "plt.title(\"Datos generados\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4qSZhkyRdf1"
   },
   "source": [
    "Intentemos ahora ajustar un modelo. Si intentamos ajustar un polinomio de grado 4, obtendremos lo que llamamos un modelo sobreajustado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p17td5_l7EBI"
   },
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste del modelo.\n",
    "Intentemos ahora ajustar un modelo. Si intentamos ajustar un polinomio de grado 3, obtendremos lo que llamamos un modelo sobreajustado. Para ellos usaremos nuevamente el \"truco\" de darle a mi modelo un polinomio de grado \"n\" en las variables. Esto nos permite ajustar un modelo lineal, puesto que en los coeficientes el modelo sigue siendo lineal. Por ende a continuacion nos generamos simplemente la matriz [vander](https://es.wikipedia.org/wiki/Matriz_de_Vandermonde) que toma cada feature y genera la progresión geométrica en cada fila:\n",
    "\n",
    "[<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/a7091e866343173f382c3aee50dff44c463f15d6\" width=\"200\" height=\"2000\" align='center'>]()\n",
    "\n",
    "\n",
    "\n",
    "Para mas info miremos la [documentación](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vander.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W1ix4M0d7EBN"
   },
   "outputs": [],
   "source": [
    "np.vander(xs, 4), ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste lineal\n",
    "Primero probemos ajustar un modelo lineal! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gfDmWBYRdf1"
   },
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "# Esta funcion de numpy genera la matriz polinomica a partir de una serie de valores\n",
    "X = np.vander(xs, 4)[:,:-1]\n",
    "y = ys\n",
    "\n",
    "model = lm.fit(X, y)\n",
    "predictions = lm.predict(X)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "\n",
    "\n",
    "plt.title(\"Muestra de datos #1\")\n",
    "plt.scatter(xs, predictions, c='r')\n",
    "print (\"r^2:\", model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nq1fWTutRdf6"
   },
   "source": [
    "El r cuadrado de \"entrenamiento\" es 0.94. Pero que pasa si \"cambiamos\" el dataset?.\n",
    "\n",
    "Si aplicamos este mismo modelo a una nueva muestra de datos generados por la misma funcion, veremos que no ajusta muy bien. Mejor dicho, vemos que ajusta notablemente peor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I0lhXbw4Rdf8"
   },
   "outputs": [],
   "source": [
    "random.seed(3)\n",
    "\n",
    "xs2, ys2 = generate_data()\n",
    "X = np.vander(xs2, 4)[:,:-1]\n",
    "predictions = lm.predict(X)\n",
    "\n",
    "plt.scatter(xs2, ys2)\n",
    "plt.title(\"Muestra de datos #2\")\n",
    "plt.scatter(xs2, predictions, c='r')\n",
    "print (\"r^2:\", model.score(X, ys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NfKiMifS7EBb"
   },
   "outputs": [],
   "source": [
    "# Veamos los coeficientes de la regresión lineal:    \n",
    "lm.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Resumen</b> \n",
    "     Vimos que durante el entrenamiento el r**2 es de 0.94 pero si cambiamos el dataset de testing baja la performance \"considerablemente\". Parece que estamos en un escenario de \"overfitting\".\n",
    "</div>\n",
    "\n",
    "Probemos utilizar alguna técnica con regularización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SljrTKA-RdgC"
   },
   "source": [
    "#  Ridge Regression\n",
    "Usemos scikit-learn para ajustar una regresión con regularización, como la que describimos en el inicio del notebook. En particular, comenzaremos por la  _ridge regression_ en inglés. ¿Hace falta normalizar los features en este caso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGqwhURqRdgE"
   },
   "outputs": [],
   "source": [
    "rlm = linear_model.Ridge(alpha=0.5, normalize=True)\n",
    "\n",
    "# Ajustamos nuevamente, esta vez con regularizacion\n",
    "X = np.vander(xs, 4)[:,:-1]\n",
    "y = ys\n",
    "\n",
    "ridge_model = rlm.fit(X, y)\n",
    "predictions = ridge_model.predict(X)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "\n",
    "plt.title(\"Muestra de datos #1\")\n",
    "plt.scatter(xs, predictions, c='r')\n",
    "print (\"r^2:\", ridge_model.score(X, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m7ZqKmU7RdgM"
   },
   "outputs": [],
   "source": [
    "X = np.vander(xs2, 4)[:,:-1]\n",
    "predictions = ridge_model.predict(X)\n",
    "\n",
    "plt.scatter(xs2, ys2)\n",
    "plt.title(\"Muestra de datos#2\")\n",
    "plt.scatter(xs2, predictions, c='r')\n",
    "\n",
    "print (\"r^2:\", ridge_model.score(X, ys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pgrlilux7EBo"
   },
   "outputs": [],
   "source": [
    "# Veamos los coeficientes de la regresión Ridge:    \n",
    "ridge_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EiRnup0RdgS"
   },
   "source": [
    "Deberían observar que el ajuste de la _ridge regression_ empeoró un poco (es decir, no fue tan bueno) en muestra #1. En cambio, la mejora fue notablemente grande en la muestra #2. Esto es porque la regularización busca prevenir el sobreajuste (overfitting).\n",
    "\n",
    "Además podemos apreciar que los coeficientes de la regresión Ridge son más chicos.\n",
    "\n",
    "Si quieren ver otro ejemplo de ridge regularization pueden leer [este ejemplo](http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html) de la documentacion oficial de scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1AwajaZ0yeRu"
   },
   "source": [
    "# Lasso\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKlG9F1x-tOA"
   },
   "source": [
    "¿Hace falta normalizar los features en este caso? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VpA1dd7WyeAI"
   },
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso(alpha=0.5, normalize=True)\n",
    "\n",
    "# Ajustamos nuevamente, esta vez con regularizacion\n",
    "X = np.vander(xs, 4)[:,:-1]\n",
    "y = ys\n",
    "lasso_model =lasso.fit(X, y)\n",
    "predictions = lasso_model.predict(X)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.title(\"Muestra de datos #1\")\n",
    "plt.scatter(xs, predictions, c='r')\n",
    "\n",
    "print (\"r^2:\", lasso_model.score(X, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IW7OBQjWzJmY"
   },
   "outputs": [],
   "source": [
    "X = np.vander(xs2, 4)[:,:-1]\n",
    "predictions = lasso_model.predict(X)\n",
    "\n",
    "plt.scatter(xs2, ys2)\n",
    "plt.title(\"Muestra de datos#2\")\n",
    "plt.scatter(xs2, predictions, c='r')\n",
    "\n",
    "print (\"r^2:\", lasso_model.score(X, ys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "po_UXQXU7EB4"
   },
   "outputs": [],
   "source": [
    "# Veamos los coeficientes de la regresión Lasso:    \n",
    "lasso_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHxsSjo_zqCN"
   },
   "source": [
    "# ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxxkb8DS-0Eu"
   },
   "source": [
    "¿Hace falta normalizar los features en este caso? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M2DM25fazzY4"
   },
   "outputs": [],
   "source": [
    "elastic_net = linear_model.ElasticNet(alpha=0.5, normalize=True)\n",
    "\n",
    "# Ajustamos nuevamente, esta vez con regularizacion\n",
    "X = np.vander(xs, 4)[:,:-1]\n",
    "y = ys\n",
    "\n",
    "elastic_net.fit(X, y)\n",
    "predictions = elastic_net.predict(X)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.title(\"Muestra de datos #1\")\n",
    "plt.scatter(xs, predictions, c='r')\n",
    "\n",
    "print (\"r^2:\", elastic_net.score(X, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sw_Ce5I3z0Qt"
   },
   "outputs": [],
   "source": [
    "X = np.vander(xs2, 4)[:,:-1]\n",
    "predictions = elastic_net.predict(X)\n",
    "\n",
    "plt.scatter(xs2, ys2)\n",
    "plt.title(\"Muestra de datos#2\")\n",
    "plt.scatter(xs2, predictions, c='r')\n",
    "print (\"r^2:\", elastic_net.score(X, ys2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NdC6FEj7ECE"
   },
   "outputs": [],
   "source": [
    "# Veamos los coeficientes de la regresión ElasticNet:\n",
    "elastic_net.coef_"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1.PRACTICA_GUIADA_Regularización_Overfitting.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
