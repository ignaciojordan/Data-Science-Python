{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "        \n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_toc\"></a> \n",
    "## Tabla de Contenidos\n",
    "\n",
    "[Intro](#section_intro)\n",
    "\n",
    "[Modelo](#section_modelo)\n",
    "\n",
    "[Ejemplo](#section_ejemplo)\n",
    "\n",
    "[Ejemplo](#section_ejemplo_2)\n",
    "\n",
    "[¿Cuándo utilizar Naive Bayes?](#section_cuando_utilizar_naive_bayes)\n",
    "\n",
    "[Naive Bayes como baseline](#section_naive_bayes_baseline)\n",
    "\n",
    "[Referencias](#section_referencias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_intro\"></a> \n",
    "## Intro\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "Supongamos que estás caminando y ves algún objeto rojo. Este objeto rojo puede ser un murciélago, un gato o una pelota. \n",
    "\n",
    "Definitivamente asumirás que será una pelota. ¿Pero por qué?\n",
    "\n",
    "Pensemos que estás haciendo una máquina y que te han asignado la tarea anterior: clasificar un objeto entre un murciélago, una pelota y un gato. \n",
    "\n",
    "Podemos pensar en crear una máquina que identifique las características del objeto y luego las mapee con sus objetos de clasificación de modo que si un objeto es un círculo, entonces será una pelota o si el objeto es un ser vivo, entonces posiblemente será un gato o si el objeto es rojo, lo más probable es que sea una pelota.\n",
    "\n",
    "¿Por qué? Porque desde nuestra infancia hemos visto una bola roja, pero no un gato rojo o un murciélago rojo.\n",
    "\n",
    "Entonces, podemos clasificar un objeto mapeando sus características individualmente con nuestro clasificador. \n",
    "\n",
    "En nuestro caso, la característica color rojo se mapeó con un murciélago, un gato y una pelota, pero el máximo valor de probabilidad lo obtuvimos mapeándola a una pelota roja, por lo tanto clasificamos ese objeto como pelota.\n",
    "\n",
    "\n",
    "\n",
    "<a id=\"section_modelo\"></a> \n",
    "## Modelo\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "Naive Bayes es una familia de clasificadores simples basados en la aplicación del Teorema de Bayes.\n",
    "\n",
    "Podemos ver un problema de clasificación de la siguiente forma, donde L son las labels y features es la matriz de features (características):\n",
    "\n",
    "$$P(L|features) = \\frac{P(features|L) P(L)}{P(features)}$$\n",
    "\n",
    "\n",
    "Este algoritmo supone que todas las features en el conjunto de datos **no están correlacionadas entre sí**, de ahí el término \"naive\".\n",
    "\n",
    "Naive Bayes es el algoritmo más simple que podemos aplicar a nuestros datos, y se utiliza principalmente para obtener la **precisión base** del dataset.\n",
    "\n",
    "\n",
    "$$P(y|x_1 ... x_n) = \\frac{P(x_1 ... x_n|y) P(y)}{P(x_1 ... x_n)}$$\n",
    "\n",
    "Usando que **las features son independientes dentro de cada clase** tenemos:\n",
    "\n",
    "$$P(x_1 ... x_n|y) = \\prod_{i=1}^{n}P(x_i|y)$$\n",
    "\n",
    "Entonces reemplazamos\n",
    "\n",
    "$$P(y|x_1 ... x_n) = \\frac{\\prod_{i=1}^{n}P(x_i|y) P(y)}{P(x_1 ... x_n)}$$\n",
    "\n",
    "Como el denominador $P(x_1 ... x_n)$ es igual para todas las clases, para realizar una predicción es suficiente con maximizar el numerador.\n",
    "\n",
    "Entonces $P(y|x_1 ... x_n)$ es proporcional a $P(y)\\prod_{i=1}^{n}P(x_i|y)$\n",
    "\n",
    "$$P(y|x_1 ... x_n) \\; \\alpha \\; P(y)\\prod_{i=1}^{n}P(x_i|y) $$\n",
    "\n",
    "La etiqueta $\\hat y$ asignada será el valor de $y$ que maximice el valor de $P(y|x_1 ... x_n)$ :\n",
    "\n",
    "$$\\hat y = \\underset y argmax \\; P(y)\\prod_{i=1}^{n}P(x_i|y)$$\n",
    "\n",
    "\n",
    "Los distintos algoritmos de Naive Bayes difieren en la distribución que suponen para $P(x_i|y)$\n",
    "\n",
    "\n",
    "\n",
    "![Image](img/naive_bayes.PNG)\n",
    "\n",
    "* Gaussian Naive Bayes: Supone distribución Gaussiana multidimensional\n",
    "\n",
    "* Naive Bayes Multinomial: Supone distribución multinomial. \n",
    "\n",
    "* Naive Bayes Bernoulli: Supone distribución bernoulli multivariada (puede haber múltiples features, pero se supone que cada una es una variable de valor binario (Bernoulli, booleana)).\n",
    "\n",
    "\n",
    "### Naive Bayes Gaussiano\n",
    "\n",
    "En este clasificador, la suposición es que los datos de cada etiqueta se extraen de una distribución gaussiana multivariada donde las features son independientes entre sí\n",
    "\n",
    "![Image](img/gaussian-NB.png)\n",
    "\n",
    "### Naive Bayes Multinomial\n",
    "\n",
    "En este clasificador se supone que las features tienen distribución multinomial simple. La distribución multinomial describe la probabilidad de observar recuentos entre varias categorías y, por lo tanto, naive bayes multinomial es más apropiado para las features que representan recuentos o tasas de recuento.\n",
    "\n",
    "La idea es exactamente la misma que antes, excepto que en lugar de modelar la distribución de datos con la distribución gaussiana de mejor ajuste, modelamos la distribución de datos con una distribución multinomial de mejor ajuste.\n",
    "\n",
    "### Naive Bayes Bernoulli\n",
    "\n",
    "Las features son booleanos independientes (variables binarias) que describen los datos de entrada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_ejemplo\"></a> \n",
    "## Ejemplo \n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "Supongamos que tenemos datos de 1000 frutas, y conocemos tres variables numéricas que las describen (features), longitud, azucar, amarillo como en la tabla que vemos a continuación\n",
    "\n",
    "![Image](img/frutas.png)\n",
    "\n",
    "¿Qué sabemos a partir de esa tabla?\n",
    "\n",
    "* 50% de las frutas son bananas\n",
    "* 30% de las frutas son naranjas\n",
    "* 20% son otras frutas\n",
    "\n",
    "Basados en el conjunto de entrenamiento también podemos decir que:\n",
    "\n",
    "* De 500 bananas, 400 (0.8) son largas, 350 (0.7) son dulces y 450 (0.9) son amarillas\n",
    "\n",
    "* De 300 naranjas 0 son largas, 150 (0.5) son dulces y 300 (1) son amarillas\n",
    "\n",
    "* De las restantes 200 frutas, 100 (0.5) son largas, 150 (0.75) son dulces y 50 (0.25) son amarillas\n",
    "\n",
    "Nos presentan ahora las características de una fruta (larga, dulce y amarilla) y queremos predecir su clase.\n",
    "\n",
    "Para ello calculamos las probabilidades para banana, naranja y otras, dados esos valores de features, y vamos a asignar esa fruta a la clase  de mayor probabilidad entres las tres.\n",
    "\n",
    "$$P(y|x_1 ... x_n) \\; \\alpha \\; P(y)\\prod_{i=1}^{n}P(x_i|y) $$\n",
    "\n",
    "Banana\n",
    "\n",
    "$P(banana|largo, amarillo, dulce) \\; \\alpha \\; P(banana)P(largo|banana)P(dulce|banana)P(amarillo|banana) $\n",
    "\n",
    "$P(banana|largo, amarillo, dulce) \\; \\alpha \\; (0.5)(0.8)(0.7)(0.9) $\n",
    "\n",
    "$P(banana|largo, amarillo, dulce) \\; \\alpha \\; 0.252 $\n",
    "\n",
    "Naranja\n",
    "\n",
    "$P(naranja|largo, amarillo, dulce) \\; \\alpha \\; P(naranja)P(largo|naranja)P(dulce|naranja)P(amarillo|naranja) $\n",
    "\n",
    "$P(naranja|largo, amarillo, dulce) \\; \\alpha \\; (0.4)(0)(0.5)(1) $\n",
    "\n",
    "$P(naranja|largo, amarillo, dulce) \\; \\alpha \\; 0 $\n",
    "\n",
    "Otras\n",
    "\n",
    "$P(otro|largo, amarillo, dulce) \\; \\alpha \\; P(otro)P(largo|otro)P(dulce|otro)P(amarillo|otro) $\n",
    "\n",
    "$P(otro|largo, amarillo, dulce) \\; \\alpha \\; (0.2)(0.5)(0.75)(0.25) $\n",
    "\n",
    "$P(otro|largo, amarillo, dulce) \\; \\alpha \\; 0.019 $\n",
    "\n",
    "\n",
    "Como la probabilidad de banana es la máxima, vamos a clasificar la nueva fruta como banana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_ejemplo_2\"></a> \n",
    "## Ejemplo\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "Vamos a usar un dataset de vinos que descargamos de https://archive.ics.uci.edu/ml/datasets/Wine para intentar predecir de qué categoria es (campo quality).\n",
    "\n",
    "Estos datos son el resultado de un análisis químico de vinos cultivados en la misma región en Italia pero derivados de tres variedades diferentes. El análisis determinó las cantidades de 13 componentes encontrados en cada uno de los tres tipos de vinos.\n",
    "\n",
    "Las categorías están codificadas como 1, 2 y 3 corresponden a calidad alta, media, o baja. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = ['quality', 'alcohol', 'malic_acid', 'ash', \n",
    "                'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids',\n",
    "                'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', \n",
    "                'hue', 'OD280_OD315_of_diluted_wines', 'proline']\n",
    "\n",
    "data = pd.read_csv(\"../Data/wine.data\", header = None)\n",
    "\n",
    "data.columns = data_columns\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos los valores que toma el campo quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.quality.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separemos el datase en la matriz de features X y el vector target Y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['quality'], axis=1)\n",
    "Y = data['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armemos los conjuntos de entrenamiento y test para nuestro clasificador, con una proporción 70-30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 1237)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo no vamos a hacer feature engineering de los datos, porque (como vamos a ver) la perfomance que obtenemos es buena aun sin escalar los features.\n",
    "\n",
    "Construimos una instancia de clasificador naive bayes gaussiano, y entrenamos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "\n",
    "gnb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el modelo entrenado, vamos a predecir las etiquetas del conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = gnb.predict(X_test)\n",
    "\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos accuracy sobre el conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(accuracy_score(Y_test, Y_pred), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos la matriz de confusión para el conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conf_mat = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "print('Confusion matrix\\n\\n', conf_mat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que sólo se equivocó en 3 casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_cuando_utilizar_naive_bayes\"></a> \n",
    "## ¿Cuándo utilizar Naive Bayes?\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Los clasificadores basados en Naive Bayes hacen fuertes supuestos sobre los datos, así que no van a tener tan buena performance si el verdadero proceso generador de los datos no cumple con los supuestos de Naive Bayes. \n",
    "\n",
    "Dicho esto, tienen las siguientes ventajas:\n",
    "* Son algoritmos muy rápidos tanto para entrenar como para predecir\n",
    "* Brindan una predicción probabilística (tenemos probabilidades para cada clase)\n",
    "* Son sencillos de interpretar\n",
    "* No requieren \"tunear\" ningún hiperparámetro\n",
    "\n",
    "\n",
    "<a id=\"section_naive_bayes_baseline\"></a> \n",
    "## Naive Bayes como baseline\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "Dado que Naive Bayes es tan fácil de optimizar y tan rápido desde el punto de vista computacional, es un buen \"baseline\" para un problema de clasificación. \n",
    "\n",
    "Si performa bien, podemos quedarnos con este modelo y si necesitamos mejorar la precisión, tenemos una línea de base sobre la cual mejorar.\n",
    "\n",
    "Naive Bayes tiende a funcionar bien en las siguientes situaciones:\n",
    "* Cuando se cumplen los supuestos (cosa que rara vez pasa en casos reales)\n",
    "* Cuando las clases están muy bien separadas y no es necesaria tanta complejidad en el modelo. \n",
    "* Cuando tenemos datos con muy alta dimensionalidad (por ejemplo text mining) donde la complejidad del modelo también es menos importante porque hay mucha información para cada observación. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_referencias\"></a> \n",
    "## Referencias\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "All about Naive Bayes\n",
    "\n",
    "https://towardsdatascience.com/all-about-naive-bayes-8e13cef044cf\n",
    "\n",
    "Wines type and quality classification exercises\n",
    "\n",
    "https://www.kaggle.com/mgmarques/wines-type-and-quality-classification-exercises\n",
    "\n",
    "Naive Bayes Classifier Explained\n",
    "\n",
    "https://towardsdatascience.com/naive-bayes-classifier-explained-54593abe6e18  \n",
    "\n",
    "Python Data Science Handbook. Jake VanderPlas. Cap 5.\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks\n",
    "\n",
    "https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.05-Naive-Bayes.ipynb\n",
    "\n",
    "StatQuest\n",
    "\n",
    "https://www.youtube.com/watch?v=O2L2Uv9pdDA\n",
    "\n",
    "https://www.youtube.com/watch?v=pYxNSUDSFH4\n",
    "\n",
    "https://www.youtube.com/watch?v=H3EjCKtlVog"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
