{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "    \n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../../common/logo_DH.svg\" align='left' width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Clasificación - KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de Contenidos\n",
    "1. <a href='#section_intro'>Introducción</a>\n",
    "2. <a href='#section_knn'>KNN</a>\n",
    "3. <a href='#section_distances'>Medidas de distancia</a>\n",
    "4. <a href=\"#section_resumen\">En resumen</a>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_intro\"></a>\n",
    "##  Introducción\n",
    "\n",
    "Hasta el momento, hemos trabajado con una de las facetas del aprendizaje supervisado, los problemas de regresión. Hacer una regresión consiste en estimar una variable _target_ continua a partir de un conjunto de características o _features_ que describen a las observaciones de nuestro conjunto de datos. Como vimos, ejemplos de este tipo de problemas podrían ser la estimación de ventas de un determinado producto a partir del presupuesto destinado a publicidad o la valuación de una propiedad en base a sus atributos (superficie total, antigüedad, ubicación, etc.).\n",
    "\n",
    "<div id=\"caja\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/haciendo_foco.png\" style=\"align:left\"/></div>\n",
    "  <br/>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>En este módulo, continuaremos trabajando con algoritmos de aprendizaje supervisado, pero concentrándonos en otro tipo de problemas, los de clasificación.</b><br></label></div>\n",
    "\n",
    "Como ya hemos anticipado, cuando hacemos una clasificación, buscamos predecir una **variable objetivo** que es **categórica** en lugar de continua. Algunos ejemplos típicos de aplicación de las técnicas de clasificación son:\n",
    "\n",
    "- Predicción de bajas de clientes a la suscripción de un servicio (modelos de *churn*)\n",
    "- Distinción de comentarios positivos y negativos en redes sociales\n",
    "- Filtros de _spam_ en servicios de correo electrónico\n",
    "- Diagnóstico de enfermedades\n",
    "- Detección de fraudes de tarjeta de crédito\n",
    "\n",
    "Más allá de que existan algoritmos específicos que se encargan de resolver estas tareas, ya que los problemas de clasificación se enmarcan dentro del _machine learning_ supervisado, veremos que la forma de trabajar es muy similar a la de los problemas de regresión, aunque ahora será necesario evaluar a nuestros modelos con métricas distintas a las del $R^2$ o el MSE. Más adelante iremos presentando diversas métricas de evaluación apropiadas para este tipo de problemas.\n",
    "\n",
    "Comenzaremos trabajando con una técnica simple de clasificación conocida como **_k_ - Nearest Neighbors (KNN)**. Vamos a describir al algoritmo y ganaremos una intuición gráfica acerca de cómo podemos utilizarlo para predecir etiquetas nuevas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_knn\"></a>\n",
    "##  KNN\n",
    "\n",
    "KNN es uno de los algoritmos más sencillos de _machine learning_. La famosa frase \"dime con quién andas y te diré quién eres\" tal vez sea la mejor explicación informal del algoritmo. Básicamente, KNN se encarga de memorizar la ubicación de cada muestra del conjunto de entrenamiento de acuerdo a los valores de sus _features_. Cuando recibe un dato nuevo, lo ubica en la posición del espacio que le corresponde según sus características y encuentra los _k_ vecinos más cercanos (_k_ es un hiperparámetro del modelo que define cuántos vecinos se van a considerar al momento de hacer las predicciones). Estos vecinos o puntos próximos son las muestras del conjunto de entrenamiento que resultan más similares a la observación que queremos clasificar. Una vez que se identificaron los _k_ vecinos más cercanos, cada uno de ellos aporta un \"voto\" a la clase a la que corresponde. La predicción queda determinada por la clase mayoritaria entre los _k_ vecinos más cercanos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/para_seguir_pensando.png\" style=\"align:left\"/></div>\n",
    "  <br/>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>Más formalmente, KNN se encarga de calcular una probabilidad para cada clase <i>c</i>:</b>\n",
    "      \n",
    "$$ p(y = c|x, D, k) = \\frac{1}{k} \\sum_{i \\in N_k(x, D)} I(y_i = c) $$\n",
    "donde $x$ es el vector de _features_ del dato a predecir, $D$ es el conjunto de datos de entrenamiento, _k_ es la cantidad de vecinos a evaluar en $D$, $N_k(x, D)$ son los índices de los _k_ vecinos más cercanos e $I(y_i = c)$ es una función indicadora que se define como\n",
    "\n",
    "$\n",
    "I(e)=\\left\\{\n",
    "            \\begin{array}{ll}\n",
    "              1\\ si\\ e\\ es\\ cierta\\\\\n",
    "              0\\ si\\ e\\ es\\ falsa\n",
    "            \\end{array}\n",
    "          \\right.\n",
    "$\n",
    "\n",
    "La clase con mayor probabilidad es la que será predicha.\n",
    "</b><br></label></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ilustrar la forma en que KNN se comporta, veamos un ejemplo sencillo con un dataset sintético generado a partir de la función  `make_classification()` de Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El método make_classification() nos permite generar datos \"de juguete\"\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# En este caso, generamos un dataset de 200 samples, cada una con 2 features, correspondientes a 2 clases distintas\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_classes=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volcamos los datos a un DataFrame\n",
    "df = pd.DataFrame(X, columns=['x1', 'x2'])\n",
    "df['y'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeccionamos el vector objetivo\n",
    "df['y'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/haciendo_foco.png\" style=\"align:left\"/></div>\n",
    "  <div style=\"float:left;width: 100%;\"><label>Notemos que el vector objetivo contiene solamente dos valores: 0 y 1. Esto se debe a que estamos trabajando con un problema de <b>clasificación binaria</b>, donde nos interesa distinguir entre <b>dos tipos de etiquetas posibles mutuamente excluyentes</b>. Naturalmente, existen problemas de clasificación no binaria o <b>clasificación multiclase</b>, donde hay <b>más de dos etiquetas posibles</b>. Si estamos ante un problema en el que las clases no son mutuamente excluyentes (<i>tagging</i> de videos de YouTube, por ejemplo), hablamos de una <b>clasificación multiclase - multietiqueta</b>, un tipo de problema un tanto más avanzado. Por el momento, nos concentraremos en la clasificación binaria.</label></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos los datos\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x='x1', y='x2', data=df, hue='y')\n",
    "plt.xlabel(f'$x_1$', fontsize=15)\n",
    "plt.ylabel(f'$x_2$', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como apreciamos en el gráfico, el cuadrante superior derecho contiene mayoritariamente datos de la clase azul ($y=0$), mientras que el cuadrante inferior izquierdo corresponde mayoritariamente a la clase naranja ($y=1$). El ejercicio será entonces entrenar un modelo que aprenda a clasificar si se trata de un punto azul o naranja en base al valor de las _features_ ($x_1, x_2$).\n",
    "\n",
    "Como siempre, antes de comenzar el entrenamiento, vamos a separar los datos en los conjuntos de _train_ y *test*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('y', axis=1), df['y'], random_state=0)\n",
    "print('Hay', y_train.shape[0], 'datos de entrenamiento y', y_test.shape[0], 'datos de testeo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hecha la separación entre _train_ y *test*, ya podemos entrenar nuestro primer KNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lIRrPKPYZs7"
   },
   "outputs": [],
   "source": [
    "# Importamos la clase KNeighborsClassifier de módulo neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4uqP_8_yYZs-"
   },
   "outputs": [],
   "source": [
    "# Instanciamos el modelo especificando el valor deseado de k con el argumento n_neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KG1HK4XkYZtC"
   },
   "outputs": [],
   "source": [
    "# Ajustamos a los datos de entrenamiento\n",
    "knn.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso, nuestro modelo se encargó de \"memorizar\" la ubicación de cada punto del _training set_. Cuando reciba un dato nuevo, simplemente lo ubicará en el espacio, indentificará los _k_=5 vecinos más cercanos a ese punto y le asignará la etiqueta correspondiente a la clase mayoritaria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/haciendo_foco.png\" style=\"align:left\"/></div>\n",
    "  <div style=\"float:left;width: 100%;\"><label>Notemos que KNN no se encarga de estimar ningún parámetro (como los $\\beta$ de la regresión lineal, por ejemplo). Otra diferencia con respecto a los modelos que hemos estudiado hasta ahora es que KNN requiere comparar cada dato de <i>test</i> contra <b>todos</b> los datos de <i>train</i> para poder detectar los vecinos más cercanos (lo cual lo hace un tanto costoso cuando estamos trabajando con datasets de gran tamaño). Por estos motivos, decimos que KNN pertence a la clase de modelos <b>no paramétricos basados en instancias</b>.\n",
    "      </label>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar las \"áreas de influencia\" de cada clase, que delimitan lo que formalmente se conoce como **fronteras de decisión**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el tamaño de la figura\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Visualizamos los datos de entrenamiento\n",
    "sns.scatterplot(x=X_train['x1'], y=X_train['x2'], hue=y_train, alpha=0.3, s=75)\n",
    "\n",
    "# Definimos una grilla de valores que abarcan todo el rango de cada variable\n",
    "x1_min, x1_max = X_train['x1'].min() - 1, X_train['x1'].max() + 1\n",
    "x2_min, x2_max = X_train['x2'].min() - 1, X_train['x2'].max() + 1\n",
    "x1, x2 = np.meshgrid(np.arange(x1_min, x1_max, .1), np.arange(x2_min, x2_max, .1))\n",
    "\n",
    "# Predecimos a partir de los valores de la grilla\n",
    "Z = knn.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "Z = Z.reshape(x1.shape)\n",
    "\n",
    "# Representamos las áreas de influencia de cada clase\n",
    "plt.pcolormesh(x1, x2, Z, cmap=ListedColormap(sns.color_palette(n_colors=2)), alpha=0.2)\n",
    "\n",
    "# Definimos los rótulos del gráfico\n",
    "plt.xlabel(f'$x_1$', fontsize=15)\n",
    "plt.ylabel(f'$x_2$', fontsize=15)\n",
    "plt.title('Fronteras de decisión', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El área azul indica que cualquier punto que se ubique dentro de ella tendrá entre sus 5 vecinos más cercanos una mayoría de puntos azules ($y = 0$), mientras que el área naranja indica que los 5 vecinos más cercanos para cualquier punto dentro de ella serán mayoritariamente naranjas ($y = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja10\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/para_seguir_pensando.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>Con esta imagen en mente, pensemos:<br>\n",
    "- ¿Qué etiqueta recibiría un dato nuevo ubicado en el punto $(x_1 = 2, x_2 = 0)$?<br>\n",
    "- ¿Y el punto $(x_1 = -2, x_2 = 0)$?</b></label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Ya estamos listos para predecir datos nuevos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TkBUHI_WYZtD"
   },
   "outputs": [],
   "source": [
    "# Predecimos etiquetas para los datos de test\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armamos una tabla con las predicciones, indicando si fueron acertadas o no\n",
    "df_pred = pd.concat([X_test, y_test, pd.Series(y_pred, index=y_test.index), y_test==y_pred], axis=1)\n",
    "df_pred.columns = ['x1', 'x2', 'y', 'y_pred', 'pred_ok']\n",
    "df_pred.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de evaluar nuestro modelo, podemos analizar gráficamente qué resultados obtuvimos. Vamos a representar los datos del _test set_ con un tamaño mayor y un color más intenso sobre el gráfico anterior, indicando con un círculo las clasificaciones correctas y con una cruz, las incorrectas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el tamaño de la figura\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "# Visualizamos los datos de entrenamiento\n",
    "sns.scatterplot(x=X_train['x1'], y=X_train['x2'], hue=y_train, alpha=0.3, s=75, legend=None)\n",
    "\n",
    "# Definimos una grilla de valores que abarcan todo el rango de cada variable\n",
    "x1_min, x1_max = X_train['x1'].min() - 1, X_train['x1'].max() + 1\n",
    "x2_min, x2_max = X_train['x2'].min() - 1, X_train['x2'].max() + 1\n",
    "x1, x2 = np.meshgrid(np.arange(x1_min, x1_max, .1), np.arange(x2_min, x2_max, .1))\n",
    "\n",
    "# Predecimos a partir de los valores de la grilla\n",
    "Z = knn.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "Z = Z.reshape(x1.shape)\n",
    "\n",
    "# Representamos las áreas de influencia de cada clase\n",
    "plt.pcolormesh(x1, x2, Z, cmap=ListedColormap(sns.color_palette(n_colors=2)), alpha=0.2)\n",
    "\n",
    "# Visualizamos las predicciones hechas sobre el test set\n",
    "sns.scatterplot(x='x1', y='x2', hue='y', edgecolor='k', linewidth=0.5,\n",
    "                alpha=1, style='pred_ok', style_order=[True, False],\n",
    "                data=df_pred, s=100)\n",
    "\n",
    "# Definimos los rótulos del gráfico\n",
    "plt.xlabel(f'$x_1$', fontsize=15)\n",
    "plt.ylabel(f'$x_2$', fontsize=15)\n",
    "plt.title('Clasificaciones realizadas', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Nada mal! De 50 observaciones del conjunto de testeo, tan sólo 3 de ellas fueron erróneamente clasificadas. En términos porcentuales, esto significa que acertamos en el 94% de las clasificaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La serie 'pred_ok' indica con booleanos si la predicción fue correcta\n",
    "# El promedio de esta serie nos indica la proporción de clasificación acertadas \n",
    "df_pred['pred_ok'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, existen métodos específicos provistos por Scikit-Learn que nos facilitan el cálculo de estas métricas de evaluación. Si queremos conocer la proporción de casos correctamente predichos, podemos utilizar la función `accuracy_score()` que importamos del módulo de métricas de Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, sabemos que la capacidad de generalización de un modelo depende de un buen ajuste a los datos de entrenamiento, sin incurrir en situaciones de sub-ajuste (sesgo) o sobre-ajuste (varianza). Con KNN, una forma de equilibrar el _trade-off_ entre sesgo y varianza es probar distintos valores del hiperparámetro _k_ hasta encontrar el adecuado. Observemos primero qué ocurre con las fronteras de decisión conforme varía el valor de _k_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una grilla de plots para representar las distintas fronteras de decisión\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(20,10), sharex=True, sharey=True)\n",
    "ax = ax.ravel()\n",
    "fig.suptitle('Fronteras de decisión', fontsize=20)\n",
    "\n",
    "# Iteramos sobre distintos valores de k\n",
    "for i, k in enumerate([1, 5, 25, 50, 100, 150]):\n",
    "    # Visualizamos los datos de entrenamiento\n",
    "    sns.scatterplot(x=X_train['x1'], y=X_train['x2'], hue=y_train, alpha=0.3, s=75, ax=ax[i])\n",
    "    \n",
    "    # Definimos una grilla de valores que abarcan todo el rango de cada variable\n",
    "    x1_min, x1_max = X_train['x1'].min() - 1, X_train['x1'].max() + 1\n",
    "    x2_min, x2_max = X_train['x2'].min() - 1, X_train['x2'].max() + 1\n",
    "    x1, x2 = np.meshgrid(np.arange(x1_min, x1_max, .1), np.arange(x2_min, x2_max, .1))\n",
    "\n",
    "    # Instanciamos y entrenamos un KNN con n_neighbors=k\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predecimos a partir de los valores de la grilla\n",
    "    Z = knn.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "    Z = Z.reshape(x1.shape)\n",
    "\n",
    "    # Representamos las áreas de influencia de cada clase\n",
    "    ax[i].pcolormesh(x1, x2, Z, cmap=ListedColormap(sns.color_palette(n_colors=2)), alpha=0.2)\n",
    "    ax[i].set_title(f'k={k}')\n",
    "    ax[i].set_xlabel(f'$x_1$')\n",
    "    ax[i].set_ylabel(f'$x_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar en estos gráficos, al cambiar el valor de _k_, se modifican las fronteras de decisión del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja10\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/para_seguir_pensando.png\" style=\"align:left\"/> </div>\n",
    "  <br><br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>¿Dónde se identifican situaciones de sub-ajuste y dónde de sobre-ajuste?</b></label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder comprender qué ocurre con el dilema entre sesgo y varianza conforme varía _k_, conviene pensar en las situaciones extremas:\n",
    "\n",
    "- De mínima, el valor de _k_ no puede ser inferior a 1, puesto que de ser igual a 0 no habría ningún vecino contra el que comparar a los datos nuevos (tampoco tendría mucho sentido hablar de identificar los -2 vecinos más cercanos, por ejemplo). Cuando _k_ vale 1, el modelo va a estar \"pegándose\" demasiado a cada punto del set de entrenamiento, lo que lleva a una pérdida considerable de generalidad. Las \"islas\" azules dentro del área naranja que se observan en la primera figura son una clara representación de esta situación. Por esto, **solemos asociar valores demasiado bajos de _k_ con una alta varianza del modelo**.\n",
    "- De máxima, _k_ no puede ser mayor al número se _samples_ del conjunto de entrenamiento, ya que estaríamos pidiéndole al modelo que evalúe más puntos de los que conoce. Si _k_ = _n_, el modelo sólo aprende a predecir la clase mayoritaria en el _training set_, de ahí que en la última figura de la grilla observemos que el área azul cubre todo el gráfico. Por esto, **valores demasiado altos de _k_ suelen asociarse a un alto sesgo del modelo**.\n",
    "\n",
    "**Hallar un valor adecuado de _k_ es un aspecto crucial al momento de trabajar con KNN. Para ello, será necesario trabajar con un esquema de _cross-validation_ que nos permita elegir un _k_ óptimo en base a múltiples pruebas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_distances\"></a>\n",
    "## Medidas de distancia\n",
    "\n",
    "Por lo que vimos hasta el momento, sabemos que KNN debe identificar las _k_ observaciones del _training set_ más parecidas o cercanas a cada muestra nueva para poder predecir una etiqueta asociada. Pero, ¿cómo definimos qué significa \"estar cerca\"? Un aspecto importante de esta cuestión es elegir una métrica de distancia que resulte apropiada.\n",
    "\n",
    "Por defecto, KNN trabaja con la [**distancia euclídea**](https://es.wikipedia.org/wiki/Distancia_euclidiana), ya que funciona bien para la mayoría de los problemas. **En dos dimensiones, la distancia euclídea equivale al teorema de Pitágoras**. Para un espacio de _n_ dimensiones, la fórmula de la distancia euclídea entre un punto $P = (p_1, p_2, ..., p_n)$ y otro punto $Q = (q_1, q_2, ..., q_n)$ es:\n",
    "\n",
    "$$ d_{(P, Q)} = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + ... (p_n - q_n)^2} = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2} $$\n",
    "\n",
    "Si observamos los argumentos de la clase `KNeighborsClassifier`, encontraremos dos que especifican esta medida de distancia:\n",
    "\n",
    "```python\n",
    "KNeighborsClassifier(p=2, metric='minkowski')\n",
    "```\n",
    "\n",
    "En efecto, la distancia euclídea es un caso particular de la [**distancia de Minkowski**](https://en.wikipedia.org/wiki/Minkowski_distance), que para dos puntos $X = (x_1, x_2, ..., x_n)$ e $Y = (y_1, y_2, ..., y_n)$ se define como:\n",
    "\n",
    "$$ D_{(X, Y)} = (\\sum_{i=1}^{n} |x_i - y_i|^p)^{\\frac{1}{p}} $$\n",
    "\n",
    "Podemos apreciar que la distancia de Minkowski depende del parámetro _p_, cuyo valor define algunas métricas de distancia comúnmente utilizadas:\n",
    "\n",
    "- Cuando _p_ = 1, la distancia de Minkowski adopta el nombre de [**geometría del taxista o distancia Manhattan**](https://es.wikipedia.org/wiki/Geometr%C3%ADa_del_taxista). El nombre se debe a que la distancia entre dos puntos se calcula como la suma de las diferencias absolutas de sus coordenadas, tal como la distancia que un taxi debe recorrer en una ciudad con un callejero cuadriculado.\n",
    "- Cuando _p_ = 2, la distancia de Minkowski equivale a la distancia euclídea.\n",
    "\n",
    "Esta ilustración muestra la diferencia entre la distancia euclídea (línea verde) y la distancia de Manhattan (líneas roja, azul y amarilla):\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/Manhattan_distance.svg/800px-Manhattan_distance.svg.png\" width=\"350\">\n",
    "<h5><center><a href=\"https://es.wikipedia.org/wiki/Geometr%C3%ADa_del_taxista\">Fuente: Wikipedia<a></center></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja10\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/para_seguir_pensando.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>Si cada \"cuadra\" de este callejero cuadriculado tiene una longitud de 1:<br>\n",
    "- ¿Qué línea es más larga, la roja, la azul o la amarilla?<br>\n",
    "      - ¿Cuánto mide la línea verde?<br></b></label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya que KNN trabaja computando distancias, **un paso importante del preprocesamiento de los datos consiste en estandarizar las variables, de forma tal de garantizar que las distintas escalas no afecten al cálculo de distancias**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_resumen\"></a>\n",
    "## En resumen\n",
    "\n",
    "<br>\n",
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/kit_de_salida.png\"/> </div>\n",
    "  <div style=\"float:left;width: 85%;\"><label>Hemos tenido nuestro primer acercamiento a los problemas de clasificación, en los que buscamos predecir una variable categórica o cualitativa en lugar de cuantitativa (como en los problemas de regresión). Presentamos un algoritmo de clasificación de los más sencillos, <b>KNN</b>, que se encarga de predecir una etiqueta para una observación en particular a partir de la clase mayoritaria entre los <i>k</i> vecinos más cercanos. Generamos una intuición visual acerca de cómo varían las fronteras de decisión del algoritmo en función del hiperparámetro <i>k</i>, el cual resulta crucial para encontrar un equilibrio adecuado entre sesgo y varianza. También, hemos visto algunas de las métricas de distancia más comunes, como la <b>distancia euclídea</b> y la <b>distancia de Manhattan</b> (ambos casos particulares de la <b>distancia de Minkowski</b>).</label></div>\n",
    "</div>\n",
    "\n",
    "Si bien en esta notebook utilizamos datos sintéticos, esto fue sólo a los fines de la presentación del algoritmo. En la próxima notebook, trabajaremos sobre un dataset real para entrenar un **KNN** que aprenda a distinguier tumores benignos de malignos, un modelo que podría ser de mucha ayuda en medicina al momento de diagnosticar pacientes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
