{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Componentes Principales (PCA)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_toc\"></a> \n",
    "\n",
    "## Tabla de Contenidos\n",
    "\n",
    "[Intro](#section_intro)\n",
    "\n",
    "[PCA para descubrir variables latentes](#section_latentes)\n",
    "\n",
    "[PCA para reducir ruido](#section_ruido)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_intro\"></a> \n",
    "## Intro\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "    \n",
    "El análisis de componentes principales (PCA) es una de las técnicas más famosas para la reducción de dimensiones, extracción de características y visualización de datos. \n",
    "\n",
    "PCA se define por la transformación de un espacio vectorial de alta dimensión en un espacio de baja dimensión. \n",
    "\n",
    "Consideremos la visualización de datos de 10 dimensiones, no es posible mostrar efectivamente la forma de una distribución de datos de una dimensionalidad tan alta. PCA proporciona una forma eficiente de reducir la dimensionalidad (es decir, de 10 a 2) permitiendo visualizar la forma de la distribución de datos. \n",
    "\n",
    "PCA también es útil en modelos de clasificación robustos donde se proporciona un número considerablemente pequeño de datos de entrenamiento de alta dimensión. Al reducir las dimensiones de los conjuntos de datos de aprendizaje, PCA proporciona un método eficaz y eficiente para la descripción y clasificación de datos.\n",
    "    \n",
    "En resumen, PCA tiene una variedad de aplicaciones:\n",
    "1. Reducción de la dimensionalidad\n",
    "2. Visualización\n",
    "3. Eliminación el ruido\n",
    "4. Generación nuevos features en el dataset\n",
    "\n",
    "**Feature extraction**\n",
    "\n",
    "Supongamos que tenemos diez variables independientes. En la extracción de características, creamos diez variables independientes \"nuevas\", donde cada variable independiente \"nueva\" es una combinación de cada una de las diez variables independientes \"antiguas\". \n",
    "Creamos estas nuevas variables independientes de una manera específica y ordenamos estas nuevas variables según cuán bien representan los datos originales.\n",
    "\n",
    "¿Dónde entra en juego la reducción de dimensionalidad? \n",
    "\n",
    "Mantenemos tantas variables independientes nuevas como queramos, pero eliminamos las \"menos importantes\". \n",
    "\n",
    "Debido a que ordenamos las nuevas variables según cuán bien representan los datos originales, sabemos qué variable es la más importante y la menos importante. Pero, y aquí está el truco, debido a que estas nuevas variables independientes son combinaciones de las anteriores, aún conservamos las partes más valiosas de nuestras antiguas variables, ¡incluso cuando eliminamos una o más de estas \"nuevas\" variables!\n",
    "\n",
    "El análisis de componentes principales es una técnica para la extracción de características, por lo que combina nuestras variables de entrada de una manera específica, ¡entonces podemos descartar las variables \"menos importantes\" mientras conservamos las partes más valiosas de todas las variables! \n",
    "\n",
    "Como beneficio adicional, cada una de las \"nuevas\" variables después de PCA son **independientes** entre sí. Esto es un beneficio porque los supuestos de un modelo lineal requieren que nuestras variables explicativas sean independientes entre sí. Si decidimos ajustar un modelo de regresión lineal con estas \"nuevas\" variables, este supuesto necesariamente se cumplirá.\n",
    "\n",
    "**¿Cuándo debemos usar PCA?**\n",
    "\n",
    "* ¿Deseamos reducir el número de variables, pero no podemos identificar qué variables no son explicativas?\n",
    "\n",
    "* ¿Deseamos asegurar que las variables sean independientes entre sí?\n",
    "\n",
    "* ¿Estamos cómodos haciendo que las variables independientes sean menos interpretables?\n",
    "\n",
    "Si la respuesta es \"sí\" a las tres preguntas, entonces PCA es un buen método. Si la respuesta a la pregunta 3 es \"no\", no debemos usar PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loadings y scores\n",
    "\n",
    "PCA se basa en la descomposición de una matriz de features X en dos matrices V y U:\n",
    "\n",
    "<img src='img/hl_pca_matmult.png' align='left'/>\n",
    "\n",
    "Las dos matrices V y U son ortogonales. \n",
    "\n",
    "La matiz V se llama **loadings**, y la matriz U se llama **scores**. \n",
    "\n",
    "Los loadings pueden pensarse como el peso de cada variable original para cada componente principal. \n",
    "\n",
    "La matriz U tiene los datos originales representados en el sistema de coordenadas definido por las componentes principales (un sistema de coordenadas rotado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_latentes\"></a> \n",
    "## PCA para descubrir variables latentes\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "Vamos ver un ejemplo presentado en *An Introduction to Statistical Learning - with Applications in R* de Gareth James, Daniela Witten, Trevor Hastie y Robert Tibshirani.\n",
    "\n",
    "En este ejemplo vamos a ver datos de arrestos en ciudades de USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/USArrests.csv', index_col=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos la media y varianza de las variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Media de las variables: \")\n",
    "print(df.mean(axis=0))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print(\"Varianza de las variables: \")\n",
    "print(df.var(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizamos los datos utilizando StandardScaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sclr = StandardScaler()\n",
    "df_std = pd.DataFrame(std_sclr.fit_transform(df), index=df.index, columns=df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos nuevamente la media y varianza de las variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Media de las variables: \")\n",
    "print(df_std.mean(axis=0))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Observamos nuevamente la varianza de las variables:\n",
    "print(\"Varianza de las variables: \")\n",
    "print(df_std.var(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos un objeto de la clase PCA. Al no especificar en el argumento el número de componentes, PCA va a conservar todos los CP.\n",
    "\n",
    "Calculamos las componentes principales con el método `fit` sobre los datos normalizados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_arrests = PCA()\n",
    "pca_arrests.fit(df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un DataFrame con los loadings.\n",
    "\n",
    "Recordemos que los loadings representan el peso de cada variable original para cada componente principal\n",
    "\n",
    "En este caso podemos ver que:\n",
    "\n",
    "* la variable Assault es la que más contribuye a la pimera componenete principal\n",
    "\n",
    "* la variable UrbanPop es la  que más contribuye a la segunda componenete principal\n",
    "\n",
    "Miramos el valor absoluto (módulo) del loading para decidir cuál es el que más contribuye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_loadings = pd.DataFrame(pca_arrests.components_.T, index=df.columns, columns=['PC1', 'PC2', 'PC3', 'PC4'])\n",
    "pca_loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que un \n",
    "\n",
    "* conjunto de vectores es ortonormal, si es un conjunto ortogonal y la norma de cada uno de sus vectores es igual a 1.\n",
    "\n",
    "* dos vectores son ortogonales si su producto interno da 0.\n",
    "\n",
    "Verificamos la ortonormalidad de los componentes principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_loadings.T.dot(pca_loadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribimos los datos originales normalizados en el sistema de coordenadas del espacio generado por las componenentes principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(pca_arrests.fit_transform(df_std), columns=['PC1', 'PC2', 'PC3', 'PC4'],\\\n",
    "                      index=df_std.index)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la media y varianza de estas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Media de los CP: \")\n",
    "print(df_pca.mean(axis=0))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Observamos la varianza de las variables:\n",
    "print(\"Varianza de los CPs: \")\n",
    "print(df_pca.var(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a usar como features de nuestro conjunto de datos sólo dos variables: PC1 y PC2.\n",
    "\n",
    "Y representamos cada registro del dataset original usando como coordena x el valor de PC1 y como coordenada y el valor de PC2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax1 = plt.subplots(figsize=(9,7))\n",
    "\n",
    "ax1.set_xlim(-3.5,3.5)\n",
    "ax1.set_ylim(-3.5,3.5)\n",
    "\n",
    "# Ploteamos a los Estados en el espacio de los Componentes Principales 1 y 2\n",
    "for i in df_pca.index:\n",
    "    ax1.annotate(i, (df_pca.PC1.loc[i], -df_pca.PC2.loc[i]), ha='center')\n",
    "\n",
    "# Ploteamos las líneas de referencia\n",
    "ax1.hlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n",
    "ax1.vlines(0,-3.5,3.5, linestyles='dotted', colors='grey')\n",
    "\n",
    "ax1.set_xlabel('Primer Componente Principal')\n",
    "ax1.set_ylabel('Segundo Componente Principal')\n",
    "    \n",
    "# Creamos ejes secundarios\n",
    "ax2 = ax1.twinx().twiny() \n",
    "\n",
    "ax2.set_ylim(-1,1)\n",
    "ax2.set_xlim(-1,1)\n",
    "ax2.tick_params(axis='y', colors='orange')\n",
    "ax2.set_xlabel('Vectores de loadings de los Componentes Principales', color='orange')\n",
    "\n",
    "# Ploteamos a las variables originales en relación a los Componentes Principales 1 y 2\n",
    "for i in pca_loadings[['PC1', 'PC2']].index:\n",
    "    ax2.annotate(i, (pca_loadings.PC1.loc[i], -pca_loadings.PC2.loc[i]), color='orange')\n",
    "\n",
    "# Plot vectors\n",
    "ax2.arrow(0,0,pca_loadings.PC1[0], -pca_loadings.PC2[0])\n",
    "ax2.arrow(0,0,pca_loadings.PC1[1], -pca_loadings.PC2[1])\n",
    "ax2.arrow(0,0,pca_loadings.PC1[2], -pca_loadings.PC2[2])\n",
    "ax2.arrow(0,0,pca_loadings.PC1[3], -pca_loadings.PC2[3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma podemos ver gráficamente la relación entre las variables originales y los primeros 2 componentes principales.\n",
    "\n",
    "\n",
    "Veamos la varianza explicada de cada componente principal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_arrests.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el ratio la varianza explicada por cada componente principal\n",
    "\n",
    "Vemos que\n",
    "\n",
    "* la primera componente principal explica el 62% de la varianza de los datos\n",
    "\n",
    "* la segunda componente principal explica el 25% de la varianza de los datos\n",
    "\n",
    "* la tercera componente principal explica el 9% de la varianza de los datos\n",
    "\n",
    "* la cuarta componente principal explica el 4% de la varianza de los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_arrests.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos el porcentaje de varianza explicada en función de la cantidad de componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "\n",
    "plt.plot([1,2,3,4], pca_arrests.explained_variance_ratio_, '-o', label='Componente individual')\n",
    "plt.plot([1,2,3,4], np.cumsum(pca_arrests.explained_variance_ratio_), '-s', label='Acumulado')\n",
    "\n",
    "plt.ylabel('Porcentaje de Varianza Explicada')\n",
    "plt.xlabel('Componentes Principales')\n",
    "plt.xlim(0.75,4.25)\n",
    "plt.ylim(0,1.05)\n",
    "plt.xticks([1,2,3,4])\n",
    "plt.legend(loc=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este gráfico podemos observar que usando sólo dos features (PC1 y PC2) explicamos más del 80% de la varianza del conjunto original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_ruido\"></a> \n",
    "## PCA para reducir ruido\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "PCA puede ser usado también como \"filtro de ruido\". La intuición es la siguiente: cada componente con varianza mucho más grande que el ruido debería verse relativamente poco o nada afectado por dicho ruido. Entonces, si reconstruimos los datos usando solamente los componentes de mayor varianza, debería ser posible conservar la mayor parte de la señal y descartar la mayor parte del ruido.\n",
    "\n",
    "Vamos a usar en este ejemplo el dataset digits que provee `sklearn.datasets`\n",
    "\n",
    "El dataset de dígitos consiste en imagenes de 8x8 píxeles de dígitos del 0 al 9 escritos a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para plotear los dígitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        \n",
    "        ax.imshow(data[i].reshape(8, 8),\n",
    "                  cmap='binary', interpolation='nearest',\n",
    "                  clim=(0, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos la función con los datos de dígitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a introducir ruido en esto datos sumando errores aleatorios que se distribuyen normal con media 0 y desvío 4.\n",
    "\n",
    "Y veamos cómo quedan los datos con ruido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "noisy = np.random.normal(digits.data, 4)\n",
    "plot_digits(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos instanciar un objeto PCA que explique el 50% de la varianza.\n",
    "\n",
    "La cantidad de componenetes necesaria para explicar el 50% de la varianza resultó ser 5, mientras que el dataset original tiene 64 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_digits = PCA(0.50)\n",
    "pca = pca_digits.fit(noisy)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos los componentes del dataset con ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.transform(noisy)\n",
    "components.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apliquemos ahora la transformación inversa, que a partir de los componentes reconstruya los valores de las x originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = pca.inverse_transform(components)\n",
    "plot_digits(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque el dataset resultado tiene menos definición que el original, vemos que el ruido fue eliminado.\n",
    "\n",
    "Observemos también que en esta visualización tenemos imágenes representadas con 12 features en lugar de las 64 que definian el datatset original. Es decir, descartamos 52 componentes de nuestro dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA para visualizar datos\n",
    "\n",
    "Vamos a proyectar los datos del dataset de digitos en las primeras dos componenetes principales, y vamos a usar esta proyección para visualizar los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_digits_vis = PCA(n_components=2)\n",
    "projected = pca_digits_vis.fit_transform(digits.data)\n",
    "print(digits.data.shape)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función para plotear los dígitos en 2 dimensiones generados por PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits_pca(projection, numbers):\n",
    "    \n",
    "    colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n",
    "          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlim(projection[:,0].min(), projection[:,0].max())\n",
    "    plt.ylim(projection[:,1].min(), projection[:,1].max())\n",
    "\n",
    "    for i in range(len(projection)):\n",
    "        plt.text(projection[i,0], projection[i,1], str(numbers[i]),\n",
    "                color=colors[numbers[i]], fontdict={'weight':'bold', 'size':9})\n",
    "        plt.xlabel('Primer Componente Principal')\n",
    "        plt.ylabel('Segundo Componente Principal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploteamos los dígitos proyectados con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits_pca(projected, digits.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que PCA logró encontrar alguna estructura en los datos pero no llegó a separar bien a la mayoría de los números. Por ejemplo, el segundo componente parece distinguir bien entre 0 y 1. \n",
    "\n",
    "Un punto importante a remarcar es que estamos ploteando usando las etiquetas de los registros. Es decir, tenemos más información que la que tendríamos en un problema típico de aprendizaje no supervisado. \n",
    "\n",
    "Veamos cuál sería el resultado del ploteo sin las etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.xlim(projected[:,0].min(), projected[:,0].max())\n",
    "plt.ylim(projected[:,1].min(), projected[:,1].max())\n",
    "\n",
    "for i in range(len(projected)):\n",
    "    plt.scatter(projected[i,0], projected[i,1], color='b', s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos intuir algún tipo de estructura o separación entre grupos, pero evidentemente los datos son demasiado complejos y no lineales para que PCA pueda capturar correctamente la estructura y separarlos en grupos distintos (acorde a su etiqueta)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "* User guide https://scikit-learn.org/stable/modules/decomposition.html#pca\n",
    "\n",
    "* Documentación https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "* Principal Component Analysis Explained Visually https://setosa.io/ev/principal-component-analysis/\n",
    "\n",
    "* http://www.statistics4u.info/fundstat_eng/cc_pca_loadscore.html\n",
    "\n",
    "* Python Data Science Handbook, Jake VanderPlas https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
