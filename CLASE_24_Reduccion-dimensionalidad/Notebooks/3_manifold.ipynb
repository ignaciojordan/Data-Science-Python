{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_toc\"></a> \n",
    "\n",
    "## Tabla de Contenidos\n",
    "\n",
    "[Intro](#section_intro)\n",
    "    \n",
    "[Isomap](#section_isomap)    \n",
    "    \n",
    "[T-SNE](#section_tsne)        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_intro\"></a> \n",
    "## Intro\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "**El arte de la simplificación**\n",
    "\n",
    "Para comprender manifold learning, es útil hacer algunas analogías de cómo nuestros cerebros simplifican el conocimiento. \n",
    "\n",
    "Piensen en la última vez que explicaron un tema complejo a un amigo o colega. \n",
    "\n",
    "Lo más probable es que no hayan intentado transmitir todo su conocimiento del tema, que probablemente les llevó años adquirir. \n",
    "\n",
    "En su lugar, decidieron presentar los elementos más relevantes para que el destinatario de la explicación pudiera tener una idea general sobre el tema: decidieron simplificar.\n",
    "\n",
    "La simplificación es un proceso cognitivo que nos ayuda a aprender sobre temas complejos. La ciencia cognitiva detrás de la simplificación es, irónicamente, bastante compleja e involucra muchos subprocesos heterogéneos. \n",
    "\n",
    "Entre las muchas cosas que se hacen cuando se trata de simplificar un tema, hay dos que son muy relevantes para el concepto de aprendizaje múltiple:\n",
    "\n",
    "a) Tratamos de determinar las partes más importantes del tema del conocimiento y omitimos el resto.\n",
    "\n",
    "b) Utilizamos analogías para simplificar y ayudarnos a comprender temas específicos.\n",
    "\n",
    "La combinación de simplificación y analogías es el núcleo de manifold learning.\n",
    "\n",
    "**Manifold learning**\n",
    "\n",
    "Un manifold es una estructura matemática fascinante que abstrae una región conectada en la que cada punto está asociado con un conjunto de puntos en su vecindario. \n",
    "\n",
    "Debido a las conexiones, los puntos en un manifold pueden transformarse en otros puntos con un esfuerzo mínimo. \n",
    "\n",
    "En el contexto del aprendizaje automático, se utilizan manifolds para representar un número conectado de puntos de datos que se pueden modelar como transformaciones desde un espacio de dimensiones superiores.\n",
    "\n",
    "Confundidos????? :) Es más simple de lo que parece. \n",
    "\n",
    "La suposición clave de manifold learning es que en una estructura de alta dimensión, la información más relevante se concentra en un pequeño número de manifolds de baja dimensión. \n",
    "\n",
    "Esto se conoce en la teoría del aprendizaje automático como la hipótesis de manifold e incluye dos puntos principales: **distribución de datos y conectividad**.\n",
    "\n",
    "Una parte de la hipótesis del manifold supone que la distribución de probabilidad en conjuntos de datos como imágenes o texto está altamente concentrada. \n",
    "\n",
    "Un ejemplo clásico para ilustrar ese punto es generar un texto significativo seleccionando al azar palabras de un texto largo. \n",
    "\n",
    "Obviamente, es muy poco probable que esto funcione porque la distribución de oraciones coherentes en lenguaje natural es un espacio muy pequeño en el conjunto de datos de las combinaciones de letras del texto original.\n",
    "\n",
    "El segundo elemento de la hipótesis de manifold es la conectividad que implica que los puntos de datos relevantes están conectados a otros puntos de datos relevantes. \n",
    "\n",
    "Podemos ver claramente en imágenes en las que se puede obtener un píxel utilizando una transformación simple de otro píxel en su vecindario.\n",
    "\n",
    "Manifold learning se basa en la hipótesis de manifold para extraer conocimiento relevante de un conjunto de datos de alta dimensión mediante la transformación en estructuras manifold de una dimensión inferior. \n",
    "\n",
    "Esta técnica reduce drásticamente los costos computacionales en muchos modelos de deep learning y se ha convertido en un elemento cada vez más importante de las soluciones de deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "Manifold learning se ha convertido en una aplicación de la geometría y, en particular, de la geometría diferencial para aprendizaje automático. \n",
    "\n",
    "Manifold learning es simplemente usar las propiedades geométricas de los datos de alta dimensionalidad para implementar las siguientes cosas:\n",
    "\n",
    "1. Clustering: encontremos grupos de puntos similares. \n",
    "\n",
    "Dado $\\{X_1, ..., X_n\\}$ un conjunto de features, construyamos una función $f: X  \\rightarrow \\{1, ..., k\\}$ donde dos puntos \"cercanos\" deben estar en el mismo grupo.\n",
    "\n",
    "2. Reducción de la dimensionalidad: proyectemos puntos en un espacio de dimensionalidad menor mientras preservamos la estructura. \n",
    "\n",
    "Dado $\\{X1, ..., Xn\\}$ en $R ^ D$, construyamos una función $f: R ^ D \\rightarrow R ^ d$ donde d < D donde se preserva la noción de \"cercanía\".\n",
    "\n",
    "3. Semi-supervisado, supervisado: Dados puntos etiquetados / no etiquetados, creemos una función de etiquetado. \n",
    "\n",
    "Dado $\\{(X1, Y1), ..., (Xn, Yn)\\}$, construyamos $f: X \\rightarrow Y$ donde dos puntos \"cercanos\" tengan la misma etiqueta.\n",
    "\n",
    "La noción de \"cercanía\" se refina utilizando la distribución de los datos. Hay algunos frameworks donde esto se logra con:\n",
    "\n",
    "1. Punto de vista probabilístico: la densidad acorta las distancias\n",
    "\n",
    "2. Punto de vista del cluster: los puntos en las regiones conectadas comparten las mismas propiedades\n",
    "\n",
    "3. Punto de vista manifold: la distancia debe medirse \"a lo largo\" de los manifold\n",
    "\n",
    "4. Versión mixta: dos puntos \"cercanos\" son aquellos conectados por un camino corto que atraviesa regiones de alta densidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos dos funciones:\n",
    "* una función para plotear los dígitos en 2 dimensiones generados por Manifold con su etiqueta asociada\n",
    "* una función para plotear los dígitos en 2 dimensiones generados por Manifold sin etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits_manifold(projection, numbers):\n",
    "    \n",
    "    colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n",
    "          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlim(projection[:,0].min(), projection[:,0].max())\n",
    "    plt.ylim(projection[:,1].min(), projection[:,1].max())\n",
    "\n",
    "    for i in range(len(projection)):\n",
    "        plt.text(projection[i,0], projection[i,1], str(numbers[i]),\n",
    "                color=colors[numbers[i]], fontdict={'weight':'bold', 'size':9})\n",
    "        plt.xlabel('Dimensión 1')\n",
    "        plt.ylabel('Dimensión 2')\n",
    "        \n",
    "def plot_digits_manifold_no_labels(projection):      \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlim(projection[:,0].min(), projection[:,0].max())\n",
    "    plt.ylim(projection[:,1].min(), projection[:,1].max())\n",
    "\n",
    "    for i in range(len(projection)):\n",
    "        plt.scatter(projection[i,0], projection[i,1], color='b', s=10)        \n",
    "        \n",
    "digits = load_digits()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_isomap\"></a> \n",
    "## Isomap\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/manifold.html#isomap\n",
    "\n",
    "Instanciamos y fiteamos isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = Isomap(n_neighbors=7, n_components=2)\n",
    "digits_iso = isomap.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploteamos los dígitos proyectados con Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits_manifold(digits_iso, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits_manifold_no_labels(digits_iso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_tsne\"></a> \n",
    "## T-SNE\n",
    "\n",
    "[volver a TOC](#section_toc)\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/manifold.html#t-distributed-stochastic-neighbor-embedding-t-sne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos y fiteamos T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "digits_tsne = tsne.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploteamos los dígitos proyectados con T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits_manifold(digits_tsne, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits_manifold_no_labels(digits_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que T-SNE logra capturar mejor la estructura de los datos y separar mejor los diferentes dígitos al proyectar en 2 dimensiones.\n",
    "\n",
    "Y tanto T-SNE como Isomap separan mejor los grupos que PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "https://medium.com/@jrodthoughts/the-art-of-simplification-manifold-learning-dfc885fb74e3\n",
    "\n",
    "https://towardsdatascience.com/manifold-learning-the-theory-behind-it-c34299748fec\n",
    "\n",
    "https://scikit-learn.org/stable/modules/manifold.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
