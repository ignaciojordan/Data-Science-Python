{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "    \n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../../common/logo_DH.svg\" align='left' width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística\n",
    "## Tabla de Contenidos\n",
    "\n",
    "- [1. Introducción](#intro)\n",
    "- [2. Repaso de regresión lineal](#lineal)\n",
    "- [3. Repaso de probabilidad](#proba)\n",
    "- [4. Regresión logística](#logistica)\n",
    "    - [4.1. Regresión logística con Scikit-Learn](#sklearn)\n",
    "    - [4.2. Estimación de los coeficientes](#coef)\n",
    "    - [4.3. Umbral de decisión](#umbral)\n",
    "    - [4.4. Clasificación multiclase](#multiclase)\n",
    "- [5. Caso de uso](#caso)\n",
    "    - [5.1. Introducción al problema](#caso_intro)\n",
    "    - [5.2. Análisis y exploración del dataset](#caso_eda)\n",
    "    - [5.3. Manipulación de datos](#caso_dummies)\n",
    "    - [5.4. Ajuste del modelo](#caso_entrenamiento)\n",
    "    - [5.5. Statsmodels](#caso_stats)\n",
    "- [6. Documentación](#docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Introducción\n",
    "Hasta ahora nos veníamos concentrando en entrenar, validad y mejorar modelos que permitieran generar una predicción para una observación a partir de un conjunto de variables que la caracterizaban.<br>\n",
    "<u>Ejemplo</u>: predecir el precio de una propiedad en función de las características de la misma. En este caso la variable objetivo (target) es el valor numérico en dólares o pesos del precio de la propiedad y las variables explicativas son la cantidad de ambientes, los metros cuadrados, el barrio, etc.<br>\n",
    "Para resolver esta tarea ya vimos algunos modelos como la regresión lineal y distintos tipos de estrategias para validarlos y mejorarlos, como por ejemplo feature engineering y regularización. \n",
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/para_seguir_pensando.png\" style=\"align:left\"/> </div>\n",
    "  <div style=\"float:left;width: 85%;\"><label><i>Para recordar: ¿Qué es y para qué se usa cada una?</i><br></label></div>\n",
    "Si bien todo lo visto hasta ahora tiene varias diferencias, ventajas y desventajas, siempre hay algo en común: <b>la variable objetivo es numérica</b>.<br>\n",
    "Como vimos la clase anterior, existen algunos problemas en los que el objetivo es poder predecir el valor de una <b>variable categórica</b>. Esto implica que tengamos que redifinir (un poco) el problema. El objetivo a grandes rasgos seguirá siendo el mismo: predicir una característica de una observación en función del resto de sus características (variables independientes --> variable objetivo/dependiente).La diferencia es que ahora el resultado de la predicción no puede tomar cualquier valor, sino que está acotado a un número finito de valores posibles. Este tipo de problemas son de <b>clasificación</b>.\n",
    "##### <u>Ejemplo práctico</u>\n",
    "Tenemos una serie de artículos de un diario y debemos saber si los mismos pertenecen a la sección de Economía, Deportes o Policiales. Sería muy fácil para una persona leer cada artículo y <i>clasificarlo</i> en función del texto, pero queremos automatizarlo con un modelo de aprendizaje automático. Planteemos el problema:\n",
    "- Features: el título y el texto del artículo.\n",
    "- Target: la sección del diario a la que pertenece.\n",
    "\n",
    "¿Qué es lo que hace que este sea un problema de clasificación? Simplemente el hecho de que la variable objetivo tenga 3 valores posibles: Economía (E), Deportes (D) o Sociedad (S) y ningún otro.<br>\n",
    "El objetivo del modelo que entrenemos, será <b>estimar la probabilidad P</b> de que el artículo leído pertenezca a cada sección.<br>\n",
    "Si tenemos 100 artículos de Economía (E), 100 de Deportes (D) y 100 de Sociedad (S) y sacamos uno al azar, tendremos\n",
    "$P(\\text{sección=E}) = \\frac{1}{3}$, \n",
    "$P(\\text{sección=D}) = \\frac{1}{3}$, \n",
    "$P(\\text{sección=S}) = \\frac{1}{3}$, lo cual no es muy útil. Pero si luego de elegir el artículo al azar, leemos el título que dice \"Copa Libertadores\", sabemos que las probabilidades serán muy diferentes. Lo que cambia, es que ahora tenemos más información para hacer la clasificación ya que está <b>condicionada</b>. Sería esperable que el modelo genere $P(\\text{sección=D}|\\text{título=\"Copa Libertadores\"}) = 0.99$\n",
    "#### Generalizando\n",
    "A partir del ejemplo anterior, podemos escribir todos los problemas de clasificación como el cálculo de una probabilidad condicional:\n",
    "$$P(y_{i}=clase_{j}|x_{i}=X) = ¿?$$\n",
    "<center>¿Cuál es la probabilidad de que la observación $i$ pertenezca a la clase $j$, sabiendo el valor de sus features $x$?</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lineal\"></a>\n",
    "## Repaso de regresión lineal\n",
    "Recordemos brevemente la regresión lineal. Es un modelo que permite estimar un valor numérico a partir de una combinación lineal del valor de las features:\n",
    "$$\\hat{y_{i}}=\\beta_{0}+\\beta_{1}·x_{1}+\\beta_{1}·x_{1}+\\dots+\\beta_{p}·x_{p}$$\n",
    "Donde los $\\beta$ se estiman ($\\hat{\\beta}$) a partir del set de entreamiento. Si tenemos una única feature, la fórmula se reduce al caso de la regresión lineal <i>simple</i>:\n",
    "$$\\hat{y_{i}}=\\hat{\\beta_{0}}+\\hat{\\beta_{1}}·x_{1}$$\n",
    "Intentemos usar esta fórmula para hacer el cálculo de probabilidad que mencionamos anteriormente, utilizando como caso un ejemplo simple en el que hay que debemos entrenar un clasificador binario que permita identificar si una obsevación pertenece a la clase \"0\" o a la clase \"1\". Es decir que intentaremos estimar $P(y=1|x)$ con la siguiente fórmula:\n",
    "$$ P(y=1|x) = \\beta_{0}+\\beta_{1}·x_{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las bibliotecas que usaremos\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos unos datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la cantidad de datos que crearemos\n",
    "n = 20\n",
    "# Seteamos una semilla para obtener resultados repetibles\n",
    "np.random.seed(0)\n",
    "# Generamos las observaciones\n",
    "x = np.linspace(-5,5,n) + np.random.rand(n)\n",
    "x = x.reshape(-1,1)\n",
    "# Les asignamos una clase (0 o 1)\n",
    "y = np.zeros(n)\n",
    "for i in range(n):\n",
    "    y[i] = np.random.choice([0,1], size=1, p=[np.exp(-i/n), 1-np.exp(-i/n)])\n",
    "#Graficamos los datos\n",
    "plt.scatter(x,y,c=y, cmap='rainbow')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('clase')\n",
    "plt.yticks([0,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supongamos que este es nuestro conjunto de datos, donde:\n",
    "- `x` es un `ndarray` y es la única feature\n",
    "- `y` es un `ndarray` que contiene la clase a la que pertenece cada observación\n",
    "\n",
    "Respondé las siguientes preguntas:\n",
    "- ¿Cuántas observaciones hay?\n",
    "- ¿Cuántas hay de cada clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solución\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del gráfico podemos deducir que cuanto más grande sea el valor de x, más probable es que la observación pertenezca a la clase 1, cuando más chico sea el valor de x, menos probable es que la clase sea 1, y hay un rango intermedio de valores de x para los cuales la clase no está tan claramente definida. Intentemos plasmar eso utilizando una regresión lineal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos un objeto de la clase LinearRegression\n",
    "linear_regression = LinearRegression()\n",
    "# Ajustamos el modelo\n",
    "linear_regression.fit(x, y)\n",
    "# Imprimimos los coeficientes\n",
    "print(linear_regression.intercept_)\n",
    "print(linear_regression.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto significa que $\\hat{y}=0.332+ 0.116·x$. Grafiquemos esta recta superpuesta con los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficamos los datos\n",
    "plt.scatter(x,y,c=y, cmap='rainbow')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('clase')\n",
    "plt.yticks([0,1])\n",
    "plt.plot(np.sort(x), linear_regression.predict(np.sort(x)));\n",
    "plt.legend([\"P(y=1)\"], fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Excelente! Logramos obtener un modelo que se ajusta a lo esperado. Los valores de la recta reflejan la relación entre x y $P(y=1|x=X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PAUSA</b><br>\n",
    "Detengámonos un momento a pensar sobre lo que acabamos de hacer...<br>\n",
    "Partimos de un dataset que presentaba un problema de clasificación binaria, o sea que $y$ puede valer 0 o 1 para cada observación. Visualizando los datos, vimos que aquellas observaciones con $y=1$ tenían valores de $x$ más grandes y las observaciones con $y=0$ tenían valores de $x$ más pequeños. Luego entrenamos una regresión lineal para intentar reflejar este comportamiento y eso fue lo que obtuvimos.<br>\n",
    "Pero acá hay algo raro..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usá el modelo entrenado para predecir la clase de 2 observaciones nuevas\n",
    "x_nuevas = np.array([[-4], [11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solución\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"proba\"></a>\n",
    "### Repaso de probabilidad\n",
    "Repasemos algunos conceptos claves de las probabilidades.<br>\n",
    "Para empezar, sabemos que la probabilidad debe estar acotada en el rango [0, 1], es decir que no pueden existir probabilidades negativas ni mayores que 1.<br>\n",
    "Por otro lado, sabemos que la suma de todas las probabilidades para cada clase posible debe ser 1. Si hay 0.25 de probabilidad de que $y=0$, entonces $P(y=1) = 1-0.25=0.75$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Esto se cumple en los casos anteriores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La respuesta es <b>no</b> y se debe a que la regresión lineal simple tiene como resultado una recta que no está acotada. Puede tomar cualquier valor y esto no nos sirve para estimar una probabilidad. Lo mismo aplica para una regresión lineal múltiple con varias features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La conclusión de esta pequeña prueba, es que una regresión lineal no sirve para resolver problemas de clasificación, en los que el objetivo es obtener una estimación de probabilidades condicionales a los features de cada observación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logistica\"></a>\n",
    "## Regresión Logística\n",
    "No todo está perdido. Hay algo que podemos hacer para subsanar los problemas que mencionamos recién.<br>\n",
    "Pensemos momentáneamente en la siguiente función:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "Esta función es conocida como <i>función sigmoidea</i> y nos será de gran utilidad en unos momentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio: graficá la función sigmoidea para valores de z entre -10 y 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio: graficá la función sigmoidea para valores de z entre -100 y 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que esta función sigue cumpliendo con lo esperado para nuestro problema:\n",
    "- Se tienen valores más grandes para valores más grandes de z\n",
    "- Se tienen valores más chicos para valores más chicos de z\n",
    "- Se tienen valores \"intermedios\" para valores \"intermedios\" de z\n",
    "\n",
    "Y suma una gran ventaja: <b>la función sigmoidea está acotada en el rango (0,1).</b> No importa cuánto valga z, $\\sigma(z)$ siempre será mayor que 0 y menor que 1. Ya empezamos a ver por qué esta función nos será de utilidad para estimar probabilidades.<br>\n",
    "Quedan algunas cosas por resolver antes de poder utilizar esta función para estimar probabilidades. Para empezar, la función sigmoidea recibe una variable $z$ que es única y nosotros sabemos que nuestros features $x$ pueden ser varios (es decir que $x$ es un <i>vector</i> $\\bar{x}$). Acá es donde entra la regresión en <b>regresión</b> logística:\n",
    "- Ya sabemos obtener un valor numérico a partir de un conjunto de features utilizando la regresión lineal\n",
    "- Dicho valor no está acotado entre 0 y 1, pero la función sigmoidea sí\n",
    "- Si calculamos la función sigmoidea del valor obtenido con la regresión lineal, obtendremos un número entre 0 y 1 que podemos usar para estimar probabilidades.\n",
    "\n",
    "Formalmente escrito:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}} \\text{ está acotada en (0,1)}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "z = \\beta_{0}+\\beta_{1}·x_{1}+\\beta_{1}·x_{1}+\\dots+\\beta_{p}·x_{p} \n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "P(y_{i}=1|x=X) = \\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}·x_{1}+\\beta_{1}·x_{1}+\\dots+\\beta_{p}·x_{p})}}\n",
    "$$\n",
    "<br>\n",
    "Prestemos atención a la última ecuación sin perder de vista nuestro objetivo: poder estimar la probabilidad $P(y=1|x=X)$.<br>\n",
    "Estamos utilizando los features $x$ para obtener un valor numérico $z$ que no está acotado. Luego estamos calculando la sigmoidea de $z$, $\\sigma(z)$ para poder interpretar ese valor como una probabilidad. Ésto es lo que se conoce como <b>regresión logística</b>\n",
    "\n",
    "<a id=\"sklearn\"></a>\n",
    "### Regresión logística con Scikit-Learn\n",
    "Scikit-Learn ya tiene implementada una clase de regresión logística con todos los métodos que ya conocemos (`.fit()`, `.predict()`) con algunos agregados que iremos viendo más adelante. Esto hace que la metodología de trabajo sea la misma de siempre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la clase\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Instanciamos un objeto de esa clase\n",
    "logistic_regression = LogisticRegression()\n",
    "# Ajustamos esta instancia con los datos de entrenamiento\n",
    "logistic_regression.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos el modelo entrenado, podemos utilizarlo para hacer predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que las predicciones obtenidas son <b>las clases de pertenencia de cada observación</b> según el modelo. Pero si estábamos hablando de estimar probabilidades, ¿por qué el modelo genera etiquetas y no estima probabilidades?<br>\n",
    "La realidad es que el modelo hace ambas cosas:\n",
    "- Estima las probabilidades de pertenecer a cada clase\n",
    "- Compara esas probabilidades con un umbral\n",
    "\n",
    "Si para una observación, $P(y=1|x=X)\\ge0.5 \\text{ entonces } \\hat{y}=1$ <br>\n",
    "Si para una observación, $P(y=1|x=X)<0.5 \\text{ entonces } \\hat{y}=0$<br>\n",
    "De esta manera se logra clasificar a las observaciones en función de las probabilidades de que pertenezca a una clase. Para visualizar estas probabilidades, debemos usar el método `.predict_proba()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.predict_proba(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que el output de `.predict_proba(x)` es un `ndarray` de forma (`n_obsevaciones`, `n_clases`). Como en este ejemplo tenemos 20 observaciones y 2 clases posibles (0 o 1), ocurre que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.predict_proba(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde la columna 0 es la probabilidad de pertenencia a la clase 0 y la columna 1 es la probabilidad de pertenencia a la clase 1.<br>\n",
    "Pregunta relámpago: ¿cuánto debe valer la suma de cada fila? Verificarlo con código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumamos el valor de cada columna para cada fila\n",
    "logistic_regression.predict_proba(x).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos quedamos sólamente con la columna 1, podremos hacer la comparación que mencionamos recién:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos sólamente con la columna 1\n",
    "prob_1 = logistic_regression.predict_proba(x)[:,1]\n",
    "# Comparamos con 0.5\n",
    "prob_1 >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos con el método .predict()\n",
    "(prob_1 >= 0.5) == logistic_regression.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que es lo mismo para todos los casos obtener predicciones con el método `.predict()` que obtener la estimación de las probabilidades con `.predict_proba()` y luego compararlo con el valor de umbral 0.5.<br>\n",
    "En la práctica a veces es necesario modificar el valor de umbral por diversos motivos, por lo que no hay que perder de vista que los clasificadores en Scikit-Learn tienen el método `.predict_proba()` que nos permite jugar manualmente con el valor de umbral. Si no es necesario modificar este valor, utilizando `.predict()` obtendremos los mismo resultados que umbralizando con 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que ya tenemos las etiquetas puestas por el modelo (es decir $\\hat{y}$) debemos comparar esas predicciones con los valores reales ($y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_regression.predict(x)\n",
    "y == y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Ejercicio:</b> calcular el <i>accuracy</i> del modelo con este set de datos. Recordemos la fórmula:\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{predicciones correctas}}{\\text{casos totales}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando numpy\n",
    "(y == y_pred).sum() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando Scikit-Learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"coef\"></a>\n",
    "#### Estimación de los coeficientes\n",
    "Recordemos que el objetivo de la regresión logística es poder estimar\n",
    "$$\n",
    "P(y_{i}=1|x=X) = \\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}·x_{1}+\\beta_{1}·x_{1}+\\dots+\\beta_{p}·x_{p})}}\n",
    "$$\n",
    "Esto implica que, al igual que en la regresión lineal, debemos encontrar los valores de los $\\beta$ que ajusten mejor a los datos. En este caso, como tenemos un sólo feature ($x$), la ecuación se reduce a \n",
    "$$\n",
    "P(y_{i}=1|x=X) = \\frac{1}{1+e^{-(\\beta_{0}+\\beta_{1}·x)}}\n",
    "$$\n",
    "dónde deberemos hallar $\\hat{\\beta_{0}}$ y $\\hat{\\beta_{1}}$. Éstos coeficientes se encuentran en los atributos `.intercept_` y `.coef_` de la instancia del modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué estos valores no coinciden con los estimados por la regresión lineal?<br>\n",
    "Recordemos que la estimación de los coeficientes en la regresión lineal minimiza el error cuadrático medio (MSE).<br>\n",
    "¿Los coeficientes de la regresión logística tienen el mismo objetivo?<br>\n",
    "La respuesta es no. En el caso de los clasificadores se utilizan otras métricas y con el siguiente ejemplo veremos por qué:<br>\n",
    "- Al estimar el precio de una casa obtuvimos un valor de USD200.000, cuando su valor real era USD210.000. El error fue de 210.000 - 200.000 = USD10.000\n",
    "- Al clasificar una imágen con animales obtuvimos la categoría \"elefante\",  cuando su etiqueta real era \"jirafa\". ¿Cómo cuantificamos el error? Sólo sabemos que la predicción fue incorrecta.\n",
    "\n",
    "En los casos de clasificación debemos pensar nuevas formas de evaluar los modelos y hacernos la pregunta ¿qué esperamos de un buen clasificador?. La respuesta es bastante simple: un buen clasificador nos dará $P(y=1|x=X)$ muy cercanos a 1 para todas las observaciones donde $y=1$ y muy cercanos a cero para todas las observaciones donde $y=0$. Esto puede expresarse con la siguiente expresión matemática:\n",
    "$$\n",
    "L = \\prod_{i:y_i=1}P(y=1|x=X) · \\prod_{i:y_i=0}(1-P(y=1|x=X))\n",
    "$$\n",
    "La primer productoria hace referencia a los casos donde $y=1$ por lo que esperamos tener probabilidades cercanas a 1 y la segunda productoria hace referencia a los casos donde $y=0$ por lo que esperamos probabilidades cercanas a 0. A esta expresión se la conoce como <b>verosimilitud</b> o <b>likelihood</b> y es lo que trataremos de maximizar.<br>\n",
    "Si manipulamos un poco la fórmula anterior, la podemos reescribir de la siguiente manera:\n",
    "$$\n",
    "L = \\prod_{i=1}^{m} P(y_i=1|x_i=X)^{y_i} · (1-P(y_i=1|x_i=X))^{1-y_i}\n",
    "$$\n",
    "$$\n",
    "log(L) = log(\\prod_{i=1}^{m} P(y_i=1|x_i=X)^{y_i} · (1-P(y_i=1|x_i=X))^{1-y_i})\n",
    "$$\n",
    "$$\n",
    "LL = \\sum_{i=1}^{m} y_i·log(P(y_i=1|x_i=X))+(1-y_i)·log((1-P(y_i=1|x_i=X)))\n",
    "$$\n",
    "A esta última expresión se la conoce como <b>log-likelihood</b>, que es el logaritmo de la verosimilitud.<br>\n",
    "Analicemos los posibles casos con los que nos podemos encontrar y veamos cómo quedan los términos de la sumatoria:\n",
    "- <u>y = 1</u>\n",
    "\n",
    "Reemplacemos $y_i = 1$ en la fórmula de LL y analicemos un sólo caso (sin la sumatoria)\n",
    "$$\n",
    "1·log(P(y_i=1|x_i=X))+(1-1)·log((1-P(y_i=1|x_i=X)))\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "1·log(P(y_i=1|x_i=X))+{(0)·log((1-P(y_i=1|x_i=X)))}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "log(P(y_i=1|x_i=X))\n",
    "$$\n",
    "<br>\n",
    "Dado que $y_i = 1$, uno esperaría que $P(y_i=1|x_i=X)$ sea muy cercano a 1, por lo que $log(P(y_i=1|x_i=X))$ sería $log(\\sim1)$, que es muy cercano a 0. <br>\n",
    "Si el clasificador es malo y genera $P(y_i=1|x_i=X)$ cercanas a 0, $log(P(y_i=1|x_i=X))$ sería $log(\\sim0)$, que tiende a $-\\infty$.\n",
    "\n",
    "- <u>y = 0</u>\n",
    "\n",
    "Reemplacemos $y_i = 0$ en la fórmula de LL y repitamos el análisis\n",
    "$$\n",
    "0·log(P(y_i=1|x_i=X))+(1-0)·log((1-P(y_i=1|x_i=X)))\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "0·log(P(y_i=1|x_i=X))+{(1)·log((1-P(y_i=1|x_i=X)))}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "log((1-P(y_i=1|x_i=X)))\n",
    "$$\n",
    "<br>\n",
    "Dado que $y_i = 0$, uno esperaría que $P(y_i=1|x_i=X)$ sea muy cercano a 0, por lo que $log((1-P(y_i=1|x_i=X)))$ sería $log(1-\\sim0)$, que es muy cercano a 0. <br>\n",
    "Si el clasificador es malo y genera $P(y_i=1|x_i=X)$ cercanas a 1, $log((1-P(y_i=1|x_i=X)))$ sería $log(1-\\sim1)$, que tiende a $-\\infty$.\n",
    "<br>\n",
    "En resumen:\n",
    "- si el clasificador predice probabilidades cercanas al caso real, se suman valores cercanos a 0\n",
    "- si el clasificador predice lo contrario al caso real, se suman valores muy grandes con signo negativo\n",
    "\n",
    "El problema que Scikit-Learn resuelve por nosotros, es <b>encontrar los $\\beta$ que maximicen la verosimilitud</b>, acercando las predicciones a los valores reales de las observaciones. Este proceso es conocido como MLE, o Maximum Likelihood Estimation.\n",
    "\n",
    "<br>\n",
    "<b>NOTA IMPORTANTE</b><br>\n",
    "Al igual que en la regresión lineal, la función de costo puede ser modificada para reducir el sobreajuste agregando un término de regularización $\\lambda\\|\\beta\\|^2$. Scikit-Learn hace esto por defecto con el hiperparámetro C = 1/$\\lambda$. Para valores más grandes de C, menor grado de regularización y viceversa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"umbral\"></a>\n",
    "### Umbral de decisión\n",
    "Como mencionamos anteriormente, el modelo genera una probabilidad de pertenencia a cada clase (un número real entre 0 y 1) que luego es comparada con un umbral de decisión, generalmente 0.5, para etiquetar cada observación con una clase. Veamos qué implicancias tiene esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generaremos 30 muestras con 2 features cada una y una clase de pertenencia\n",
    "n=30\n",
    "np.random.seed(1)\n",
    "x1 = np.random.rand(n)*10-5\n",
    "x2 = np.random.rand(n)*10-5\n",
    "y = (x1-x2 >0).astype(int)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x1,x2, hue=y, s=100)\n",
    "plt.legend(['Clase 1', 'Clase 0'], loc='upper left')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Ejercicio:</b> ajustá un modelo de regresión logística al nuevo conjunto de datos e imprimí los coeficientes encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unificamos las features en un unico array\n",
    "x = np.array([x1,x2]).T\n",
    "# Ajustamos el modelo\n",
    "logistic_regression.fit(x, y)\n",
    "# Imprimimos los coeficientes\n",
    "print(logistic_regression.coef_)\n",
    "print(logistic_regression.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtuvimos un coeficiente para cada feature y uno para la ordenada al origen. Esto significa que las probabilidades se calcularán como\n",
    "$$\n",
    "P(y=1|x=X)=\\sigma(z)=\\sigma(\\beta_0+\\beta_1·x_1+\\beta_2·x_2)\n",
    "$$<br>\n",
    "$$\n",
    "P(y=1|x=X)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1·x_1+\\beta_2·x_2)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y todas aquellas probabilidades que sean mayores a 0.5 serán etiquetadas como 1 y todas las que sean menores serán etiquetadas como 0. El umbral de decisión se ubica cuando $\\sigma(z)=0.5$ y, como se ve en el gráfico, esto ocurre para $z=0$. <i>Bonus: demostrarlo analíticamente</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(np.linspace(-8,8,100), sigmoid(np.linspace(-8,8,100)), 'r');\n",
    "plt.ylabel('sigmoide(z)');\n",
    "plt.xlabel('z');\n",
    "plt.ylim([-0.2,1.2])\n",
    "plt.yticks([0,0.5,1])\n",
    "plt.xticks([0],['0'])\n",
    "plt.grid(color='k', linestyle='dashdot', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordando que $z = \\beta_0+\\beta_1·x_1+\\beta_2·x_2$, podemos hacer las siguientes operaciones:\n",
    "$$\n",
    "z = \\beta_0+\\beta_1·x_1+\\beta_2·x_2\n",
    "$$<br>\n",
    "$$\n",
    "0 = \\beta_0+\\beta_1·x_1+\\beta_2·x_2\n",
    "$$<br>\n",
    "$$\n",
    "\\frac{-\\beta_0-\\beta_1·x_1}{\\beta_2} = x_2\n",
    "$$<br>\n",
    "$$\n",
    "x_2 = - x_1 · \\frac{\\beta_1}{\\beta_2} - \\frac{\\beta_0}{\\beta_2}\n",
    "$$<br>\n",
    "La última expresión tiene la forma de una recta de $x_2$ en función de $x_1$. Si reemplazamos los coeficientes obtenidos y graficamos esa recta sobre el conjunto de datos, obtenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los datos\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.scatterplot(x1,x2, hue=y, s=100)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2');\n",
    "# Graficamos la recta\n",
    "eje_1 = np.linspace(-5,5, 10)\n",
    "beta_0 = logistic_regression.intercept_[0]\n",
    "beta_1 = logistic_regression.coef_[0][0]\n",
    "beta_2 = logistic_regression.coef_[0][1]\n",
    "eje_2 = -eje_1*beta_1/beta_2 - beta_0/beta_2\n",
    "plt.plot(eje_1, eje_2, c='r');\n",
    "plt.legend(['umbral', 'Clase 1', 'Clase 0'], loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que la recta obtenida es la que mejor separa las dos clases.\n",
    "\n",
    "<a id=\"multiclase\"></a>\n",
    "### Clasificación multiclase\n",
    "Hasta ahora estuvimos analizando un caso en el que los datos tenían sólo dos etiquetas posibles. \"Azul\" vs \"naranja\", \"violeta\" vs \"rojo\", 0 vs 1. Esto casos son de clasificación <b>binaria</b> y vimos cómo ajustar un modelo de regresión logística para resolver este problema y cómo interpretar los resultados del ajuste. Existe un gran abanico de problemas de clasificación en los que hay más de 2 formas posibles de etiquetar las observaciones y estos son llamados problemas de <b>clasificación multiclase</b>.<br>\n",
    "Supongamos que tenemos una imágen y queremos clasificar si hay un animal en ella. Estamos ante un caso de clasificación <b>binaria</b>, ya que sólo hay 2 posibilidades: hay animal, o no hay animal. Si lo que deseamos es saber qué animal hay en la foto, es un caso de clasificación <b>multiclase</b> ya que existen varias etiquetas que le podemos asignar a la foto (\"perro\", \"gato\", \"elefante\", etc. o \"sin animal\"). También puede ocurrir que en la imagen haya más de un animal y estos casos son denominados <b>multietiqueta</b>, ya que a cada observación le puede tocar más de una etiqueta.<br>\n",
    "Las implementaciones de Scikit-Learn vienen preparadas para trabajar de manera nativa con problemas multiclase y en el caso de la regresión logística el problema se encara de una manera muy simple. Si tenemos 4 etiquetas posibles, digamos \"rojo\" \"celeste\", \"verde\" y \"violeta\", podemos plantear 4 problemas binarios por separado:\n",
    "- ¿es \"rojo\"?\n",
    "- ¿es \"azul\"?\n",
    "- ¿es \"verde\"?\n",
    "- ¿es \"violeta\"?\n",
    "\n",
    "Generalizando, un problema multiclase con N etiquetas, podemos plantearlo como N problemas binarios. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un dataset de pruebas\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, # generamos 100 muestras\n",
    "                           n_features=2, n_redundant=0, # con 2 features explicativas\n",
    "                           n_classes=4, # que pueden pertenecer a 4 clases distintas\n",
    "                           n_clusters_per_class=1, # donde cada clase comprende un cluster\n",
    "                           random_state=1) # y fijamos una semilla para obtener resultados repetibles\n",
    "# Visualizamos los datos\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.scatterplot(X[:,0], X[:,1], hue=y, palette='rainbow')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilicemos la implementación de Scikit-Learn para resolver el problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En función de los valores encontrados en `y`, el objeto `logistic_regression` determina automáticamente la cantidad de clases posibles y genera las etiquetas correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de ejecutar las siguientes celdas, ¿qué resultado esperás obtener?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.predict_proba(X).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Y qué pasa con los coeficientes e interceptos de este modelo? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada una de las 4 clases tiene sus propios coeficientes e interceptos. Generalizando:\n",
    "- `.coef_` tendrá forma (N_clases, N_features)\n",
    "- `.intercept_` tendrá forma (N_clases,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En estos casos, en lugar de comparar las probabilidades (`.predict_proba`) con un valor de umbral, la etiqueta elegida será la mayor de todas las probabilidades para esa observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.predict_proba(X).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(logistic_regression.predict(X) == logistic_regression.predict_proba(X).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"caso\"></a>\n",
    "## Caso de uso\n",
    "Ahora veremos un ejemplo práctico en el que usaremos la regresión logística para clasificar admisiones de estudiantes a distintas universidades.\n",
    "\n",
    "<a id=\"caso_intro\"></a>\n",
    "### Introducción al problema\n",
    "Estamos interesados en entender cómo influyen algunas variables en las probabilidades de admisión de alumnos a distintas universidades. Las variables con las que trabajaremos son las siguientes:\n",
    "- GRE: graduate record exam\n",
    "- GPA: grade point average\n",
    "- Prestigio de la institución de proveniencia\n",
    "\n",
    "Contamos con un dataset de 400 pedidos de ingreso a 10 universidades distintas en la que cada pedido está etiquetado como 1 (admitido) o 0 (no admitido).\n",
    "\n",
    "<a id=\"caso_eda\"></a>\n",
    "### Análisis y exploración del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/binary.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos las 3 variables cuya influencia deseamos analizar y la variable objetivo (admit). Valores más altos de `gre` y `gpa` indican mejores resultados, mientras que los valores más bajos de `prestige` indican mayor prestigio de la institución de procedencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance del dataset\n",
    "df['admit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El 31% de los estudiantes son admitidos. Veamos un poco más en detalle cómo se distribuyen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='admit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo se distribuyen las observaciones en el plano gpa-gre para los distintos niveles de prestigio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"prestige\", hue=\"admit\")\n",
    "g.map(plt.scatter, \"gpa\", \"gre\", alpha=.7)\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver de forma un poco más clara, podemos hacer histogramas agrupando según el valor de admit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax_new = plt.subplots(1,3, sharey=False,figsize=(16,5))\n",
    "axes_ = df.boxplot(by='admit',ax=ax_new,return_type='axes',whis=[5,95]);\n",
    "for ax,col in zip(axes_,['gpa','gre','prestige']):\n",
    "    ax.set_ylim(df[col].min()/1.1,df[col].max()*1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que hay ligeras modificaciones en la mediana que podrían predecir el valor de admit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"caso_dummies\"></a>\n",
    "### Manipulación de datos\n",
    "Al igual que ocurre con la regresión lineal, en la regresión logística necesitamos trabajar con variables numéricas. Es por esto que debemos transformar las variables categóricas en variables dummy.<br>\n",
    "En este caso la única variable categórica que tenemos es el prestigio de la institución de proveniencia, que está expresada en forma numérica. A mayor prestigio, más bajo el número. Si esta variable ya es numérica, ¿debemos transformarla en dummies?<br>\n",
    "Recordemos que la regresión logística realiza una combinación lineal de las features. Esto implica que un GPA de 3, contribuye el doble que un GPA de 1.5 (de ahí lo lineal). ¿Podemos afirmar que una institución con prestigio 2 es el doble de prestigiosa que una con prestigio 4? La realidad es que no podemos asumir que el prestigio viene dado en una escala lineal y por lo tanto no podemos tratar la variable como numérica y debemos crear dummies para cada nivel de prestigio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos dummies para prestige\n",
    "dummies_prestige = pd.get_dummies(df['prestige'], drop_first=True, prefix='prestige')\n",
    "dummies_prestige.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unimos los dataframes\n",
    "df = pd.concat([df, dummies_prestige], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya podemos armar la matriz de features y el vector objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['gre','gpa','prestige_2','prestige_3','prestige_4']]\n",
    "y = df[\"admit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y separar los datos en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"caso_entrenamiento\"></a>\n",
    "### Ajuste del modelo\n",
    "Recordemos que la implementación de la regresión logística de Scikit-Learn aplica regularización por defecto con el hiperparámetro C, por lo que será importante estandarizar los datos antes de fitear el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utilizamos sklearn para estandarizar la matriz de Features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos el modelo\n",
    "logistic_regression.fit(X_train_scaled, y_train)\n",
    "# Y visualizamos los coeficientes\n",
    "print(logistic_regression.coef_)\n",
    "print(logistic_regression.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos predicciones con el modelo entrenado\n",
    "y_train_pred = logistic_regression.predict(X_train_scaled)\n",
    "y_test_pred = logistic_regression.predict(scaler.transform(X_test)) # Notar que debemos escalar los datos de testeo antes de realizar predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elaboramos la matriz de confusión\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_test_pred)\n",
    "sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True)\n",
    "plt.ylabel('Verdaderos')\n",
    "plt.xlabel('Predichos');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculemos el accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"caso_stats\"></a>\n",
    "### Uso de `statsmodels`\n",
    "Recordemos que Scikit-Learn es sólo una de las tantas bibliotecas que ofrecen implementaciones de modelos de aprendizaje automático. `statsmodels` es otra de ellas que ofrece un enfoque más estadístico. En esta sección utilizaremos las implementaciones de `statsmodels` para la regresión logística y repetiremos el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsmodels` pide que explicitemos si el modelo debe estimar un término independiente $\\beta_0$ o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stats = sm.add_constant(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciamos la clase\n",
    "logit = sm.Logit(y_train, X_train_stats)\n",
    "# Fiteamos el modelo\n",
    "result = logit.fit()\n",
    "# Imprimimos el resumen\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mirando el summary obtenido, ¿qué podemos decir sobre los coeficientes $\\beta_1$, $\\beta_2$ y $\\beta_3$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hagamos predicciones\n",
    "X_test_stats = sm.add_constant(scaler.transform(X_test))\n",
    "# Obtenemos las probabilidades y las comparamos con 0.5 \n",
    "y_pred_stats = result.predict(X_test_stats) > 0.5\n",
    "# Generamos la matriz de confusión\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_stats), annot=True)\n",
    "plt.ylabel('Verdaderos')\n",
    "plt.xlabel('Predichos');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculamos el accuracy\n",
    "accuracy_score(y_test, y_pred_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"docs\"></a>\n",
    "### Documentación\n",
    "- [Regresión logística con Scikit-Learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [Regresión logística con Statsmodels](https://www.statsmodels.org/stable/generated/statsmodels.discrete.discrete_model.Logit.html#statsmodels.discrete.discrete_model.Logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression.classes_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
