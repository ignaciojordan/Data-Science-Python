{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "    \n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../../common/logo_DH.svg\" align='left' width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Jerárquico\n",
    "\n",
    "En la notebook anterior vimos el algoritmo de kmeans. Una de las particularidades de kmeans, que en muchos casos puede ser un problema, es el hecho de que haya que determinar el número de clusters k a priori. Una estrategia posible era determinar el valor de k mirando distintos coeficientes que miden de alguna manera la \"calidad\" del clustering. \n",
    "\n",
    "Los algoritmos de clustering jerárquico no sólo no requieren determinar el número de clusters a priori, sino que ofrecen una visualización de los clusters en forma de arbol que resulta muy informativa, para determinar la granularidad (el nro de clusters) más apropiada.\n",
    "\n",
    "Veremos aquí algoritmos de clustering jerárquico \"aglomerativos\" o \"bottom up\" en los cuales se comienza por la resolución más fina, en la cual cada punto es un cluster, de modo que hay N clusters de tamaño 1, y se procede a fusionar los pares de clusters más cercanos iterativamente hasta tener un solo cluster de tamaño N.\n",
    "\n",
    "Este proceso se esquematiza en la siguiente figura:\n",
    "\n",
    "<img src='img/hierarchical_clustering.png' width=90%>\n",
    "\n",
    "La figura de abajo a la derecha es la visualización de los clusters que mencionábamos antes, llamada dendrograma. Allí se ve el orden en el cual fueron fusionados los clusters y también a qué distancia se encontraban al momento de unirse. Es decir, la altura de la barra azul horizontal representa la distancia entre los puntos A y C; la altura de la roja, la distancia entre B y el cluster AC.\n",
    "\n",
    "De esta manera, la visualización del dentrograma muchas veces permite ver cuántos clusters hay en los datos, aún cuando los mismos vivan en un espacio de alta dimensionalidad.\n",
    "\n",
    "Además de ser necesario definir una medida de distancia, al igual que con k-means, ahora debemos definir un criterio mediante el cual fusionaremos los clusters. A este criterio se lo llama \"linkage\" y algunas maneras de definirlo son las siguientes:\n",
    "\n",
    "<img src='img/linkage.png' width=90%>\n",
    "<img src='img/linkage_2.png' width=90%>\n",
    "\n",
    "* <b>Ward</b> es la opción por default. Tiende a generar clusters de dimensiones similares y funciona bien en la mayoría de los casos prácticos.\n",
    "\n",
    "* Si tenemos clusters con diferentes cantidades de miembros, <b>complete</b> y <b> average</b> son buenas opciones.\n",
    "\n",
    "* <b>Single linkage</b> tiende a generar clusters extendidos en los que las hojas se van agregando de a una.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo\n",
    "\n",
    "Trabajaremos nuevamente con el dataset \"mall_customers.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('../Data/mall_customers.csv')\n",
    "data.rename({'Annual Income (k$)':'Income','Spending Score (1-100)':'Spending Score'},axis=1,inplace=True)\n",
    "display(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en kmeans, el algoritmo depende de una medida de distancia entre los datos en el espacio de las features. Por este motivo es necesario llevarlas a la misma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[['Income','Spending Score']];\n",
    "\n",
    "scaler=StandardScaler();\n",
    "\n",
    "X_sc=scaler.fit_transform(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para implementar el modelo de clustering jerárquico no usaremos la librería scikit-learn sino funciones de la librería scipy, de modo que la sintaxis no involucrará los métodos \"fit\" y \"predict\" clásicos de sklearn. Concretamente aplicaremos la función **linkage** a nuestros datos y luego graficaremos los resultados con la función **dendrogram**. Hagámoslo y luego comentamos estas funciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(X_sc, 'ward');\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Index Numbers')\n",
    "plt.ylabel('Distance')\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  \n",
    "    leaf_font_size=5.,  \n",
    "    color_threshold=0,\n",
    "    truncate_mode='lastp'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La función linkage\n",
    "\n",
    "La función linkage devuelva una matriz **Z** de (n-1) filas y 4 columnas. En la iteración i-ésima, los clusters con índices **Z[i, 0]** y **Z[i, 1]** se combinan para formar el cluster **n + i**. Un cluster con un índice menor a **n** corresponde a una de las **n** observaciones originales. La distancia entre los clusters **Z[i, 0]** and **Z[i, 1]** la encontramos en **Z[i, 2]**. El cuarto valor **Z[i, 3]** representa el número de observaciones en el cluster que es está formando.\n",
    "\n",
    "#### El dendrograma\n",
    "\n",
    "Mirando el dendrograma, ¿cuántos clusters se ven en los datos?\n",
    "\n",
    "Para identificar clusters en el dendrograma debemos definir una distancia de corte en el eje vertical, de manera que nos queden ramas independientes del arbol, cada una de las cuales será un cluster. Y la distancia entre todos los pares de clusters formados será mayor que la distancia de corte. Por ejemplo si cortáramos el arbol a una altura del eje y=6, quedaría así:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sns.color_palette('hls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy \n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.title('Dendrogram')\n",
    "plt.xlabel('Data points')\n",
    "plt.ylabel('Distance')\n",
    "\n",
    "color_palette=['r','g','y','m','c'];\n",
    "\n",
    "hierarchy.set_link_color_palette(color_palette) \n",
    "\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  \n",
    "    leaf_font_size=5.,  \n",
    "    color_threshold=6,     \n",
    ")\n",
    "plt.hlines(6,0,2000,linestyle='--')\n",
    "plt.xticks([])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera quedan definidos 5 clusters, cada uno de los cuales tiene un \"diámetro\" característico cercano a 4, y que la distancia mínima entre ellos es cercana a 10. Por este motivo esta elección de número de clusters parece adecuada.\n",
    "\n",
    "Para ver la composición exacta de cada cluster usamos la función fcluster, pasando como argumento la distancia de corte o directamente el número de clusters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=fcluster(Z,5,criterion='maxclust');\n",
    "\n",
    "print('Cluster labels:\\n',labels)\n",
    "\n",
    "sns.scatterplot(x=X_sc[:,0],y=X_sc[:,1],hue=labels,legend='full',palette=color_palette);\n",
    "plt.xlabel('Income',fontsize=15);plt.ylabel('Spending Score',fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coeficiente Cofenético\n",
    "\n",
    "Este coeficiente pretende medir la calidad del clustering jerárquico a través de la correlación lineal entre las distancias originales de cada par de puntos y las distancias de los clusters que los contenían al momento de unirse:\n",
    "\n",
    "$$\n",
    "c=\\frac{\\sum_{i<j} (x(i,j)-\\bar{x})(t_{i,j}-\\bar{t})}\n",
    "{\\sqrt{\\sum_{i<j} (x(i,j)-\\bar{x})^2 \\sum_{i<j} (t(i,j)-\\bar{t})^2}}\n",
    "$$\n",
    "\n",
    "en donde $x(i,j)$ es la distancia entre los puntos i-j y $t(i,j)$ es la distancia entre los clusters que contenían al punto i y al punto j al momento de unirse.\n",
    "\n",
    "Uno esperaría que este coeficiente fuera cercano a uno, de modo que las dos distancias estén muy correlacionadas.\n",
    "\n",
    "En nuestro ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c,dists=cophenet(Z, pdist(X_sc))\n",
    "print('Cophenetic coefficient:',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(X_sc,method,ax):\n",
    "    Z = linkage(X_sc, method);\n",
    "    c,dists=cophenet(Z, pdist(X_sc));\n",
    "    ax.set_title(method+' c='+str(np.around(c,2)))        \n",
    "    ax.set_xticklabels([])\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        leaf_rotation=90.,  \n",
    "        leaf_font_size=5.,  \n",
    "        color_threshold=0,\n",
    "        truncate_mode='lastp',\n",
    "        ax=ax\n",
    "    )        \n",
    "    return  \n",
    "\n",
    "\n",
    "\n",
    "fig,axes=plt.subplots(2,2,figsize=(10,10))\n",
    "\n",
    "plot_dendrogram(X_sc,'ward',axes[0,0])\n",
    "plot_dendrogram(X_sc,'average',axes[0,1])\n",
    "plot_dendrogram(X_sc,'complete',axes[1,0])\n",
    "plot_dendrogram(X_sc,'single',axes[1,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pueden ver más ejemplos y una discusión ampliada sobre métricas de evaluación del clustering jerárquico [acá](https://joernhees.de/blog/2015/08/26/scipy-hierarchical-clustering-and-dendrogram-tutorial/#Selecting-a-Distance-Cut-Off-aka-Determining-the-Number-of-Clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
