{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "    \n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../../common/logo_DH.svg\" align='left' width=50%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTICA GUIADA: DBSCAN\n",
    "\n",
    "Hemos visto los algoritmos de k-means y clustering jerárquico. La principal debilidad de los mismos está en el hecho de asumen una geometría esférica en la distribución de los datos. \n",
    "\n",
    "Veamos un ejemplo en el que esta premisa no se cumple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN,KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist,squareform\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, label = make_moons(n_samples=400, noise=0.07, random_state=19)\n",
    "\n",
    "X= StandardScaler().fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sctr = ax.scatter(X[:,0],X[:,1], c='lightblue', edgecolor='black', s=40, alpha=0.9,\n",
    "                 cmap=plt.cm.Set1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering con kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=2, random_state=19)\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "centers=km.cluster_centers_;\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.scatter(X[y_km==0,0], X[y_km==0,1],c ='lightblue', edgecolor='black', marker='o',s=40, label='cluster 1')\n",
    "ax.scatter(X[y_km==1,0], X[y_km==1,1],c ='red', edgecolor='black', marker='o',s=40, label='cluster 2')\n",
    "ax.scatter(centers[0,0],centers[0,1] ,marker='+',c='k',s=60)\n",
    "ax.scatter(centers[1,0],centers[1,1] ,marker='+',c='k',s=60,label='centroids')\n",
    "ax.set_title('K-means clustering')\n",
    "fig.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering con DBSCAN\n",
    "\n",
    "Dbscan (Density Based Spatial Clustering of Applications with Noise) es un algoritmo que, como su nombre lo indica, identifica clusters como regiones de alta densidad de puntos. El algoritmo tiene dos hiperparámetros: \n",
    "\n",
    "* Eps ($\\epsilon$): un radio para definir la densidad local de cada punto\n",
    "\n",
    "* minPts: número mínimo de puntos dentro del radio para considerar que un punto es de tipo <i>core</i>.\n",
    "\n",
    "Se definen tres tipos de puntos:\n",
    "\n",
    "* Core: puntos que tienen más de N vecinos dentro de un radio Eps. Es decir, son puntos de alta densidad.\n",
    "\n",
    "* Border: puntos que están dentro del radio de un punto core, pero ellos no cumplen la condición de core. Son las fronteras de los clusters.\n",
    "\n",
    "* Noise: puntos que no cumplen la condición de core ni de border. Son outliers, o ruido, y no pertenecen a ningún cluster.\n",
    "\n",
    "<img src='img/dbscan.png' style='background-color:white'>\n",
    "\n",
    "#### El algoritmo se puede abstraer en estos tres pasos:\n",
    "\n",
    "\n",
    "\n",
    "1) Etiquetar cada punto como core o non-core viendo si el número de vecinos en un radio Eps es mayor a minPts.\n",
    "    \n",
    "2) Encontrar las componentes conectadas por puntos core y asignarles un número de cluster a cada una.\n",
    "\n",
    "3) Asignar el número del cluster más cercano a cada punto non-core, si se encuentra a una distancia menor a Eps. Si no, etiquetarlo como noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implementación de DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "y_dbscan = dbscan.fit_predict(X)\n",
    "y_dbscan\n",
    "\n",
    "# Lista de los puntos core y máscara para graficar\n",
    "core_samples_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
    "core_samples_mask[dbscan.core_sample_indices_] = True\n",
    "\n",
    "labels = dbscan.labels_\n",
    "unique_labels=set(labels);\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "for i,l in enumerate(unique_labels):\n",
    "    if l ==-1:\n",
    "        ax.scatter(X[labels==l,0], X[labels==l,1],c =colors[i], edgecolor='black', marker='o',s=40,label='Noise')\n",
    "    else:\n",
    "        ax.scatter(X[labels==l,0], X[labels==l,1],c =colors[i], edgecolor='black', marker='o',s=40,label='Cluster '+str(l))\n",
    "    \n",
    "ax.legend()\n",
    "ax.set_title('DBSCAN clustering');\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.3f\" % silhouette_score(X, labels))\n",
    "print(\"Calinsky-Harabasz Index: %0.3f\" % calinski_harabasz_score(X, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de DBSCAN\n",
    "\n",
    "* No es necesario definir el número de clusters a priori.\n",
    "\n",
    "* Permite encontrar clusters con forma arbitraria, no necesariamente con geometría esférica.\n",
    "\n",
    "* Al incluir la noción de ruido, dbscan el robusto ante la presencia de outliers.\n",
    "\n",
    "* Tiene sólo dos hiperparámetros, fáciles de interpretar, que se pueden definir a priori en base a conocimientos del dominio de los datos.\n",
    "\n",
    "### Desventajas \n",
    "\n",
    "* No es totalmente determinista: los puntos-borde que están a distancia menor a eps de más de un cluster podrían cambiar de etiqueta según el algoritmo de implementación.\n",
    "\n",
    "* La performance disminuye en casos en donde la densidad de puntos varía mucho según la región ya que no se pueden ajustar los parámetros minPts y eps adecuadamente para todos los clusters.\n",
    "\n",
    "* Si no se conocen bien los datos y su escala puede ser dificil determinar $\\epsilon$.\n",
    "\n",
    "### Elección de los parámetros $\\epsilon$ y minPts ()\n",
    "\n",
    "[Referencia](https://en.wikipedia.org/wiki/DBSCAN#Parameter_estimation)\n",
    "\n",
    "\n",
    "Idealmente, si se tiene conocimiento del dominio de los datos, $\\epsilon$ es una distancia típica con sentido físico y minPts es el tamaño mínimo para los clusters. Si no, existen algunas heurísticas para determinar valores razonables:\n",
    "\n",
    "* <b>MinPts:</b> Una regla de pulgar es asignar este parámetro en función del número de dimensiones $D$ en el dataset, como minPts $\\ge$ D+1. minPts = 1 no tiene sentido, ya que cada punto sería un cluster de tamaño 1.  Con minPts $\\le$ 2 el resultado es mismo que el de clustering jerárquico usando single linkage, cortando el dendrograma a una altura $\\epsilon$ . En general minPts se elige mayor a 3 y una manera usual de determinarlo es <b>minPts=2D</b>, pero puede ser necesario usar valores mayores para datasets muy grandes, ruidosos o con datos duplicados.\n",
    "\n",
    "* <b>$\\epsilon$:</b> Se puede determinar mirando un gráfico de distancias al k-ésimo vecino, con k=minPts-1. Se calcula la distancia al k-ésimo vecino más cercano de cada punto y luego se grafican estas distancias de mayor a menor. Una buena elección de $\\epsilon$ es en el codo o punto de quiebre de la curva. La elección de $\\epsilon$ dependerá fuertemente de la métrica de distancias que se use, la cual debe determinarse antes.\n",
    "\n",
    "\n",
    "<b> Ejemplo de curva de distancias k-ésimas: </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "minPts=5;\n",
    "k=minPts-1;\n",
    "\n",
    "D=squareform(pdist(X));\n",
    "\n",
    "k_distances=np.zeros(D.shape[0]);\n",
    "\n",
    "for i in range(D.shape[0]):\n",
    "    distances=np.sort(D[i]);\n",
    "    k_distances[i]=distances[k];\n",
    "k_distances=np.sort(k_distances);\n",
    "k_distances=k_distances[::-1];\n",
    "    \n",
    "plt.plot(k_distances);\n",
    "plt.xlabel('Rank');\n",
    "plt.ylabel('K-distance');\n",
    "plt.hlines([0.2,0.15],0,400,linestyles='dashed');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una elección de $\\epsilon$ entre 0.15 y 0.20 parece ser apropiada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Aquí tienen unas buenas visualizaciones de DBSCAN y KMeans:\n",
    "\n",
    "https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
