{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "try:\n",
    "    # settings colab:\n",
    "    import google.colab\n",
    "        \n",
    "except ModuleNotFoundError:    \n",
    "    # settings local:\n",
    "    %run \"../../../common/0_notebooks_base_setup.py\"\n",
    "    \n",
    "from checkpoint_evaluacion_modelos import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='../../../common/logo_DH.png' align='left' width=35%/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center><ins>EVALUACIÓN DE MODELOS EN PROBLEMAS DE CLASIFICACIÓN</ins></center></h1>\n",
    "<h1><center>Práctica guiada:</center></h1>\n",
    "<img src=\"img/01_Ev_Modelos_caratula.jpeg\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tabla_contenidos\"></a> \n",
    "## Tabla de Contenidos\n",
    "\n",
    "### <a href='#section_objetivos'>0. Objetivos de la Notebook</a>\n",
    "\n",
    "\n",
    "### <a href='#section_repaso'>1. Introducción</a>\n",
    "- #### <a href='#section_reg_vs_clas'>1.1 Regresión vs. Clasificación</a>\n",
    "- #### <a href='#section_outcomes'>1.2 Outcomes de una clasificación</a>\n",
    "- #### <a href='#section_matriz_confusion'>1.3 Matriz de confusión</a>\n",
    "\n",
    "\n",
    "### <a href='#section_metricas'>2. Métricas de evaluación</a>\n",
    "- #### <a href='#section_caso'>2.1 Presentación del caso</a>\n",
    "- #### <a href='#section_modelo'>2.2 Entrenamos y aplicamos el modelo</a>\n",
    "- #### <a href='#section_matriz_confusion_code'>2.3 Métricas: Matriz de confusión</a>\n",
    "- #### <a href='#section_accuracy'>2.4 Métricas: Accuracy</a>\n",
    "- #### <a href='#section_error'>2.5 Métricas: Error de Clasificación</a>\n",
    "- #### <a href='#section_recall'>2.6 Métricas: Sensitivity (o recall)</a>\n",
    "- #### <a href='#section_specificity'>2.7 Métricas: Specificity</a>\n",
    "- #### <a href='#section_precision'>2.8 Métricas: Precision</a>\n",
    "- #### <a href='#section_fpr'>2.9 Métricas: False positive rate (FPR)</a>\n",
    "- #### <a href='#section_f1_score'>2.10 Métricas: F1-Score</a>\n",
    "\n",
    "\n",
    "### <a href='#section_curva_roc'>3. Curva ROC</a>\n",
    "- #### <a href='#section_umbrales'>3.1 Ajustando los umbrales</a>\n",
    "- #### <a href='#section_croc'>3.2 Introducción Curva ROC y AUC</a>\n",
    "- #### <a href='#section_imp_croc'>3.3 Implementación Curva ROC y AUC</a>\n",
    "\n",
    "\n",
    "### <a href='#4.'>4. Comentarios finales</a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_objetivos\"></a> \n",
    "## 0. Objetivos de la Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja11\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 9%;\"><img src=\"../../../common/icons/haciendo_foco.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\">\n",
    "      <label>Explorar métricas de evaluación de modelos de clasificación.</label>\n",
    "  <div style=\"float:left;width: 85%;\">\n",
    "      <label>Aplicación de estas métricas con la librería:</label>    \n",
    "      <a class=\"reference internal\" href=https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics><b><code>scikit-learn</code></b></a><ul>     \n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_repaso\"></a> \n",
    "## 1. Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_reg_vs_clas\"></a> \n",
    "### 1.1 Regresión vs. Clasificación\n",
    "\n",
    "En el **Modulo 2** utilizamos métricas de [`scikit-learn`](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) para evaluar problemas de **REGRESIÓN**: situaciones en las que el **valor a predecir pertenece a una variable continua** (por ejemplo, evalúamos cuán bueno era un modelo que predecía el valor por metro cuadrado de una propiedad utilizando features tales como el tipo de propiedad, su ubicación, la cantidad de habitaciones, y los amenities con los que contaba, entre otras características de un inmueble).\n",
    "\n",
    "\n",
    "En los problemas de **CLASIFICACIÓN**, en cambio, el objetivo es **predecir la pertenencia o la probabilidad de pertenencia** de un caso a una clase. \n",
    "<img src=\"img/02_regression_vs_classification.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_outcomes\"></a> \n",
    "### 1.2 Outcomes de una clasificación\n",
    "\n",
    "Los outcomes en una clasificación se pueden dividir en cuatro clases:\n",
    "- **Falsos positivos (FP):** es una clase negativa que fue clasificada como positiva. Tomando el caso anterior de la temperatura, en el cual nuestra clase positiva es predecir si determinado día va a estar caluroso, **FP** sería predecir que va a ser calor, pero luego ese día hace frío. \n",
    "- **Falsos negativas (FN):** es una clase positiva que fue clasificada como negativa. En nuestro ejemplo, un **FN** sería predecir que va a ser frío y que luego haga calor. \n",
    "- **Verdaderos positivos (TP):** es una clase positiva clasificada correctamente. En nuestro caso, sería predecir que un día va a hacer calor, y luego realmente hace calor ese día. \n",
    "- **Verdaderos negativos (TN):** es una clase negativa clasificada correctamente. En nuestro caso, sería predecir que un día va a hacer frío, y luego realmente hace frío ese día. \n",
    "\n",
    "Recuerden que la noción de **\"positivo\"** (asociada al calor en nuestro ejemplo) o **\"negativo\"** (asociada al frío) son arbitrarias y podrían ser intercambiables en función de cuál sea el problema con el que nos enfrentamos y nuestro interés (si quisiérams saber qué días en el año serían ideales para usar la pileta, es probable que la clase **positiva** esté asociada al calor; lo contrario sucedería si quisiésemos elegir días para patinar sobre hielo.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_matriz_confusion\"></a> \n",
    "### 1.3 Matriz de confusión\n",
    "\n",
    "La **Matriz de confusión** es una tabla de doble entrada donde se describen los **resultados observados** (por ejemplo, el registro real si hizo frío o calor determinado día) **vs. resultados predichos** (las predicciones que hicimos sobre dichos días a partir de nuestro modelo). \n",
    "\n",
    "Nos permite discernir entre los casos **bien clasificados** y aquellos que fueron **erróneamente clasificados** por el modelo.\n",
    "\n",
    "A partir de la **Matriz de confusión** podemos construir las **outcomes** de la clasificación (**TP**, **TN**, **FP**, y **FN**).\n",
    "<img src=\"img/03_confusion_matriz.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Además, a partir de la **Matriz de confusión** podemos derivar las **distintas métricas** que nos vas a permitir evaluar los modelos de clasificación (siendo que cada una de ellas hace foco en distintos cuadrantes de la matriz de confusión). \n",
    "<img src=\"img/04_metrics.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "En la **próxima sección**, a partir de un problema de clasificación, veremos cómo calcular cada una de ellas (no a mano, sino usando a nuestra librería amiga [`scikit-learn`](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)) y qué es lo que cada una nos brinda como información útil para interpretar nuestros resultados y tomar decisiones al respecto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja10\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/ponete_a_prueba.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>¿Entonces... </b></label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutá esta celda...\n",
    "test_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_metricas\"></a> \n",
    "## 2. Métricas de evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_caso\"></a> \n",
    "### 2.1 Presentación del caso\n",
    "\n",
    "El **objetivo** de esta sección es analizar en la práctica las **medidas de evaluación para modelos de clasificación** mencionadas. Para ello trabajaremos tratando de **predecir la probabilidad de que un empleado deje una empresa**. Disponemos de un dataset que contiene los siguientes campos:\n",
    "1. Última evaluación\n",
    "2. Cantidad de proyectos en los que trabajó\n",
    "3. Promedio de horas mensuales trabajadas\n",
    "4. Tiempo en la compañía\n",
    "5. Si sufrió un accidente de trabajo\n",
    "6. Si tuvo una promoción en el último año\n",
    "7. Nivel salarial\n",
    "\n",
    "El objetivo, entonces, es **predecir la probabilidad** de que $P(left=1 | X)$. Nos enfrentamos a un problema que va a tener dos clases que arbitrariamente vamos a etiquetar como **positiva** refiriéndonos a la persona que deja la empresa y **negativa** como la persona que permanece\n",
    "<img src=\"img/01_tow_labels.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para empezar, importamos las librerías que hemos visto en módulos pasados y cargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/HR_comma_sep.csv')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_modelo\"></a> \n",
    "### 2.2 Entrenamos y aplicamos el modelo\n",
    "\n",
    "Utilizamos una [`LogisticRegession`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) junto con otras herramientas de la librería **scikit-learn** para preparar los datos, instanciar el modelo, entrenarlo y aplicarlo: [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) y [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armamos la matriz de predictores ($X$) y el target ($y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', \n",
    "              'time_spend_company', 'Work_accident', 'promotion_last_5years']\n",
    "X = df[train_cols]\n",
    "y = df['left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos el split entre train y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos sklearn para standarizar la matriz de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/kit_de_salida.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>HINT para no olvidar:</b> recuerden que siempre que estamos aplicando transformaciones sobre nuestros datos que dependen de estos (como la normalización), las transformaciones se entrenan con los datos de <b>train</b> sin incluir los de <b>test</b>, y sobre estos últimos se aplica la transformación ya entrenada. Por eso usamos <code>scaler.fit_transform(X_train)</code> para <b>train</b> pero sólo el método <code>scaler.transform(X_test)</code> para <b>test</b></label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instanciamos una Regresión Logística (podríamos haber utilizado otros modelos, pero la idea es mantener el problema simple y concentrarnos en entender cómo se evalúan los modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/kit_de_salida.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>HINT para no olvidar:</b> ¿Qué significa el argumento <b>C</b> en la:<a class=\"reference internal\" href=https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html><b><code>LogisticRegession</code>?</b></a>\n",
    "   <div style=\"float:left;width: 85%;\"><label>Revisá la documentación para recordar por qué le estamos pasando como argmento un número tan grande (pista: tiene algo que ver con el tema de Regularización)<label>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos y aplicamos nuestro modelo sobre datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/05_warning.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "**_¿Warning...? Keep Calm and Code On_** (but check carefully to understand what is going on): \n",
    "\n",
    "Como ya hemos visto en los módulos pasados, los [`Warning messages`](https://docs.python.org/3/library/warnings.html) nos alertan sobre situaciones respecto de la herramienta que estamos usando pero que, en general, no implican que nuestro código haya fallado. Estos mensajes suelen advertir, como en este caso, de cambios que se van a producir en próximas versiones de la librería y que sí van a impactar en el funcionamiento de nuestro código. \n",
    "\n",
    "En este caso, el **warning** se silencia indicando explícitamente el **solver** de la regresión logística cuando instanciamos el modelo:  `clf = LogisticRegression(C=1e10, solver='lbfgs')`. Si no te apareció ningún warning, entonces... **_Keep Calm and Code On_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos las predicciones realizadas por nuestro modelo, pasemos a ver cómo nos fue utilizando las distintas métricas para evaluar los resultados de un modelo de clasificación!\n",
    "\n",
    "<img src=\"img/06_predicciones2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_matriz_confusion_code\"></a> \n",
    "### 2.3 Métricas: Matriz de confusión\n",
    "\n",
    "Antes de estimar las distintas métricas, vamos a ver cómo generar la **Matriz de confusión** con **sklearn**.\n",
    "<img src=\"img/03_confusion_matriz_aux.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "**Documentación:** [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn.metrics.confusion_matrix) .\n",
    "\n",
    "**Y un ejemplo de su uso:** https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html .\n",
    "\n",
    "Básicamente, como mencionamos anteriormente, la **Matriz de confusión** es una tabla de contingencia que tabula la distribución de los casos analizados en función de su **valor real (\"observado)** y su **valor estimado por el modelo (\"predicho\")**.\n",
    "\n",
    "En `confusion_matrix` es importante recordar que el primer argumento corresponde a los **valores observados** y el segundo a los **valores predichos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos del módulo [`metrics`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn.metrics.confusion_matrix) la función [`confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn.metrics.confusion_matrix), y lo aplicamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo se lee esto?** : En las filas están representados los datos observados (`y_test`). En las columnas se representan los datos predichos por el modelo (`y_pred`).\n",
    "\n",
    "Si recordamos qué significaba cada uno de los cuadrantes de la **matriz de confusión**: \n",
    "<img src=\"img/03_confusion_matriz.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja10\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/ponete_a_prueba.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>¿Entonces... </b></label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutá esta celda...\n",
    "test_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **matriz de confusión** nos está aportando la siguiente información: \n",
    "* **True Positives (TP):** hemos predicho correctamente que el empleado se va (clase positiva, 1) **(283)**\n",
    "* **True Negatives (TN):** hemos predicho correctamente que el empleado se queda (clase negativa, 0) **(3464)**\n",
    "* **False Positives (FP):** hemos predicho que el empleado se iba pero se queda **(305)**\n",
    "* **False Negatives (FN):** hemos predicho que el empleado se queda pero se va **(898)**\n",
    "\n",
    "Con estos valores podríamos computar las distintas métricas para evaluar el modelo como vimos en las fórmulas de la **sección 1.3**, pero en su lugar vamos a utilizar directamente las herramientas que nos provee `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ahora vamos a asignarle variable a la matriz de confusión para algunos cálculos que haremos más adelante (en los que `sklearn`) no nos va a ayudar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_accuracy\"></a> \n",
    "### 2.4 Métricas: Accuracy\n",
    "\n",
    "El **Accuracy** de los datos de **test** se calcula como la proporción de casos correctamente clasificados de los datos de **test** sobre el total de casos de **test**.\n",
    "<img src=\"img/06_accuracy.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Documentación:** [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html).\n",
    "\n",
    "**Y un ejemplo de su uso:** https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score .\n",
    "\n",
    "En `accuracy_score` el primer argumento corresponde a los **valores observados** y el segundo a los **valores predichos**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos del módulo [`metrics`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn.metrics.confusion_matrix) la función [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html), y lo aplicamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy=', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué significa esto?** : este caso, encontramos que el **76% (aprox.)** de los casos —en el **test set**— han sido correctamente clasificados.\n",
    "\n",
    "Ahora bien, ¿qué tan bueno es este clasificador? ¿Qué significa que podamos clasificar correctamente a esta proporción de casos?\n",
    "\n",
    "Una primera forma de comenzar a responder esta pregunta es comparar la performance con un clasificador bien simple y (casi) trivial: se lo suele llamar **\"clasificador nulo\"** y consiste simplemente en predecir solamente teniendo en cuenta la clase más frecuente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el **24%** de los casos en el **test** **set** son 1, es decir que se irán de la empresa. Por ende, la proporción de 0 (es decir casos que no sevan de la empresa), la podemos calcular de la siguiente manera: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.0 - y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué significa esto?** Esto significa que si usáramos un modelo simple en el que para **todos los casos** de los **datos de test** utilizáramos la etiqueta de la clase mayoritaria, nuestro **accuracy** sería de **76%**. En este caso, estaríamos prediciendo bien todos los casos negativos (es decir, los que se quedan en la empresa) pero estaríamos errando en los casos positivos (todos los que se van de la empresa). \n",
    "\n",
    "Y si comparamos el resultado que obtendríamos con este **modelo simple**, nos damos cuenta que, aunque en un primer momento el resultado de nuestro **modelo de regresión logística** parecía muy bueno (**76%**), no es superior a un modelo **dummy** que se limita a etiquetar a todos los casos con la etiqueta de la clase mayoritaria: \n",
    "\n",
    "<img src=\"img/07_dummy_result.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "**Y en este caso, esto nos sucedió porque...** \n",
    "\n",
    "<img src=\"img/08_unbalancced_clasess.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "Los problemas de **clases desbalanceadas** son muy típicos en ciertos problemas tales como la detección de default financiero (donde un porcentaje bajo de las personas con créditos terminan no pagando sus cuotas... en general), la detección de spams, etc.\n",
    "\n",
    "En estos problemas, el **accuracy** suele ser alto porque esta métrica hace foco tanto en la detección correcta de **TN** como de **TP**. Pero quizás nuestro problema de negocio requiere que nos concentremos más en la detección de los **TP** y no de los **TN**. Más adelante en el curso vamos a tener una clase especial para a aprender estrategias para resolver estos problemas de **clases desbalanceadas** y que los algoritmos que apliquemos no vayan por el camino más fácil de detectar la clase mayoritaria. \n",
    "\n",
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/kit_de_salida.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>Take home message:</b> Antes de evaluar un modelo, chequear el desbalanceo de clases. Si las clases están muy desbalanceadas, es probable que el <b>accurracy</b> no sea una métrica confiable. Para eso entonces, vamos a ver a continuación otras métricas que nos van a informar mejor de cómo está funcionando este modelo.</label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_error\"></a> \n",
    "### 2.5 Métricas: Error de Clasificación\n",
    "\n",
    "Es, básicamente, el complemento del accuracy. Cuantifica el error total comentido por el clasificador. No tenemos un constructor de sklearn, sino que lo podemos estimar \"a mano\"\n",
    "<img src=\"img/07_error rate.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1 - accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_recall\"></a> \n",
    "### 2.6 Métricas: Sensitivity (o recall)\n",
    "\n",
    "**Sensitivity** o **recall** o **true positive rate (TPR)** mide la capacidad (qué tan \"sensible\" es) del modelo de detectar los **verdaderos positivos (TP)** sobre todos los casos que son positivos **(FN+TP o todos los positivos)**. En nuestro ejemplo, del total de personas que se van, sería evaluar cuántas logra identificar correctamente el modelo.\n",
    "<img src=\"img/08_sensitivity.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Documentación:** [`recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html).\n",
    "\n",
    "**Y un ejemplo de su uso:** https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py .\n",
    "\n",
    "En `recall_score` el primer argumento corresponde a los **valores observados** y el segundo a los **valores predichos**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos del módulo [`metrics`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn.metrics.confusion_matrix) la función [`recall_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html), y lo aplicamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué significa esto?** Que nuestro modelo está teniendo un bajo rendimiento en detectar a las personas que se van en relación a las que realmente se van de la empresa. Podríamos pensar que de cada 100 personas que se van, nuestro modelo tan sólo detectará 23 y el resto las clasificará como personas que se quedan. \n",
    "\n",
    "Comparado con el **accuracy_score**, esta medida nos da una mejor idea de cómo está funcionando nuestro modelo en relación al interés de nuestro negocio, y no va a estar afectada por los problemas de desbalanceo de clases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_specificity\"></a> \n",
    "### 2.7 Métricas: Specificity\n",
    "\n",
    "**Specificity** o **true negative rate (TNR)** mide la capacidad de detectar los **\"verdaderos negativos (TN)** sobre el total de casos que son negativos **(TN+FP)**. Es decir, nos da información sobre qué tan específico o selectivo es el modelo al predecir las instancias positivas.\n",
    "<img src=\"img/09_specificity.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Esta métrica no la podemos calcular con herramientas de sklearn, así que tenemos que hacerlo \"a mano\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué significa esto?** Esto significa que nuesto modelo es muy específico, es decir que cometió pocos errores de falsos positivos (es decir que pocas veces dijo que un empleado se iba a ir, cuando en realidad ese empleado no se fue de la compañía).\n",
    "\n",
    "**Specificity** o **TNR** puede ser importante, por ejemplo, en el contexto en el que estamos evaluando administrar una droga con fuertes efectos secundarios a una población de potenciales pacientes y, en ese caso, queremos evitar cometer errores en los que le administremos la droga a alguien que **NO** es un paciente. En ese escenario, queremos que nuestro modelo sea muy **específico**.\n",
    "\n",
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/kit_de_salida.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>Tener en cuenta que:</b> priorizar modelos con <b>mayor especificidad</b>, puede generar <b>una pérdida en la sensibilidad</b> (es decir, mientras más seguro quiero estar en que selecciono solamente los que son pacientes, puede ser que más pacientes los identifique como personas sanas). Lo contrario ocurre si se promueve el escenario inverso.</label></div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_precision\"></a> \n",
    "### 2.8 Métricas: Precision\n",
    "\n",
    "**Precision** o **positive predictive value (PPV)** mide qué tan **\"preciso\"** es el clasificador al predecir las instancias positivas. Es decir, cuando el clasificador predice un valor positivo..., ¿qué tan frecuentemente es correcta esta predicción?\n",
    "\n",
    "Se calcula como el número de **predicciones correctas** sobre el **número total de predicciones**\n",
    "<img src=\"img/10_precision.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "Esta métrica nos permite evaluar el peso de los **Falsos Positivos (FP)**\n",
    "\n",
    "**Documentación:** [`precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html).\n",
    "\n",
    "**Y un ejemplo de su uso:** https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py .\n",
    "\n",
    "En `precision_score` el primer argumento corresponde a los **valores observados** y el segundo a los **valores predichos**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos del módulo [`metrics`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn.metrics.confusion_matrix) la función [`precision_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html), y lo aplicamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/kit_de_salida.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>Tener en cuenta que:</b> aunque parecen lo mismo <b>especificidad</b> y <b>pecisión</b> son muy distintos. Aunque en ambos están pesando los <b>falsos positivos (FP)</b>,en la <b>especificidad</b> evaluamos los errores que cometemos en relación a los <b>TN</b> (que en nuestro ejemplo es muy alto, 0.91), mientras que con la <b>precisión</b> evaluamos cuán precisos somos cada vez que decimos que una persona se va a ir de la empresa, con lo que estamos evaluando los errores en relación al <b>TP</b> (y en nuestro caso no somos muy buenos porque sólo en la mitad de las veces que nuestro modelo dice que la persona se va a ir de la empresa, realmente eso sucede)</label></div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué significa esto?** Que nuestro modelo no es muy preciso. De cada 100 predicciones positivas que realiza el modelo (es decir, predice que un empleados se va a ir de la compañía), sólo en 48 acierta.\n",
    "\n",
    "Dependiendo del problema de negocio que estemos evaluando, quizás nuestro objetivo sea tener una precisión muy alta. Es decir, que de cada 100 predicciones positivas, la mayoría sean verdaderas y cometamos un bajo porcentaje de **falsos positivos (FP)**.\n",
    "\n",
    "<br>\n",
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/kit_de_salida.png\" style=\"align:left\"/> </div>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>Tener en cuenta que:</b> aunque parecen lo mismo <b>especificidad</b> y <b>pecisión</b> son muy distintos. Aunque en ambos están pesando los <b>falsos positivos (FP)</b>,en la <b>especificidad</b> evaluamos los errores que cometemos en relación a los <b>TN</b> (que en nuestro ejemplo es muy alto, 0.91), mientras que con la <b>precisión</b> evaluamos cuán precisos somos cada vez que decimos que una persona se va a ir de la empresa, con lo que estamos evaluando los errores en relación al <b>TP</b> (y en nuestro caso no somos muy buenos porque sólo en la mitad de las veces que nuestro modelo dice que la persona se va a ir de la empresa, realmente eso sucede)</label></div>\n",
    "</div> \n",
    "<img src=\"img/11_presicion.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "Además, ahora sabemos que **no es lo mismo** ser **preciso** que tener buen **accuracy**:\n",
    "<img src=\"img/14_accuracy_vs_precise.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja10\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/ponete_a_prueba.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>¿Entonces... </b></label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutá esta celda...\n",
    "test_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_fpr\"></a> \n",
    "### 2.9 Métricas: False positive rate (FPR)\n",
    "\n",
    "**False positive rate (FPR)** se calcula como el númoer de clases incorrectas predichas, sobre el total de clases negativas. \n",
    "<img src=\"img/12_false_positive_rate.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "El mejor **FPR** es 0, mientras que el peor es 1. \n",
    "\n",
    "Se calcula como `1-specificity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1-specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué significa esto?** Este valor es el complemento de la especificidad, así que su interpretación es también el complement. En este caso nos está indicand cuántos errores cometió de falsos positivos (es decir que pocas veces dijo que un empleado se iba a ir, cuando en realidad ese empleado no se fue de la compañía)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_f1_score\"></a> \n",
    "### 2.10 Métricas: F1-Score\n",
    "\n",
    "Es un promedio armónico entre precision y recall.\n",
    "<img src=\"img/14_f1_score.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "La ventaja de usar la media armónica (en vez de la media aritmética), es que, como se observa en la figura anterior, el resultado del f1-score no es sensible a valores altos de una de las dos variables (**recall** o **precision**). Por otro lado, no todos los valores extremos son ignorados, ya que los que son muy bajos si tienen peso en el resultado final. \n",
    "\n",
    "Es decir que para tener un `f1-score` alto, es necesario que tanto el **recall** como la **precision** sean altas, mientras que un `f1-score` bajo puede ser el resultado de un valor bajo en por lo menos una de estas métricas o en ambas a la vez.\n",
    "\n",
    "**Documentación:** [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).\n",
    "\n",
    "**Y un ejemplo de su uso:** https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py .\n",
    "\n",
    "En `f1_score` el primer argumento corresponde a los **valores observados** y el segundo a los **valores predichos**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos del módulo [`metrics`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html?highlight=confusion%20matrix#sklearn.metrics.confusion_matrix) la función [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html), y lo aplicamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué significa esto?** En este caso nuestro modelo tiene un bajo valor de `f1_score` porque tanto el recall como la precision tienen valores bajos. Es decir, que ni es bueno identificando las clases positivas, ni tampoco es preciso aciéndolo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_curva_roc\"></a> \n",
    "## 3. Curva ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_umbrales\"></a> \n",
    "### 3.1 Ajustando los umbrales\n",
    "\n",
    "Hasta ahora hemos trabajado con el resultado de las predicciones nos indique la pertenencia o no a una clase. Así nuestro resultado `y_pred` contiene valores 0 (se queda en la empresa) ó 1 (se va de la empresa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si pasamos `y_pred` a Serie, podemos usar los métodos `.unique()` o `.value_counts()` para ver los elementos que tiene y corroborar la afirmación anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, también podemos obtener como output de nuestro modelo un **valor de probabilidad de pertenencia a una clase**. Es decir, en vez de tener valores 0 ó 1, como en nuestro caso, vamos a tener valores que van a ir de 0 a 1 y que, mientras más cerca de 0 nos van a indicar que es más probable que pertenezca a la clase negativa, y mientras más cerca de 1 nos va a indicar que ese caso está cerca de la clase positiva. \n",
    "\n",
    "Para acceder a estos valores de probablidad, tenemos que utilizar el método [`.predict_proba()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) del modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos utilizar el método `.predict_proba()` y observemos el tipo de output que nos devuelve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(y_pred_proba.shape, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué pasó con nuestras predicciones?** Vamos paso a paso: \n",
    "- El método `.predict_proba()` nos devuelve una array en el cual aparecen dos probabilidades de cada instancia del test set: $p(y=0)$ y $p(y=1)$, en ese orden.\n",
    "- Ahora el output de nuestras predicciones tiene 2 columnas. La primera columna refiere a la probabilidad de un caso de pertenecer a la clase 0 (negativa), mientras que la segunda columna refiere a la probabilidad de un caso de pertenecer a la clase 1 (positiva). \n",
    "- En nuestro ejemplo, en el primer caso, la primera columna tiene un valor de 0.96, lo que significa que el modelo estimo con una probabilidad muy alta su pertenencia a la clase negativa (es decir, está muy seguro de que es 0). \n",
    "- Fijensé que en nuestro ejemplo, el último caso sería lo opuesto, ya que tiene un valor de probabilidad muy alto en la segunda columna, que se asocia a la clase 1. \n",
    "- Es decir, que el `.predict_proba()` nos va a dar como output un array que va a tener tantas filas como casos y tantas columnas como etiquetas de las que tenga que estimar su probabolidad de pertenenecia. \n",
    "- Dado que en nuestro ejemplo estamos abordando un problema binario, la información de una sóla columna nos permite deducir la probabilidad de la otra (es decir que conociendo la probabilidad asociada a una clase, podemos conocer el de la otra). \n",
    "\n",
    "Cuando utilizamos el método `.predict` con nuestros modelos, lo que está ocurriendo es que se está asumiendo que si $p(y=1) > 0.5$, entonces, la predicción del modelo será que $y=1$. Fijémonos qué sucede si aplicamos este threshold a nuestros datos de probablidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos quedamos con una de las columnas, ya que al ser una clasificación binaria, la información de una nos permite deducir la información de la otra. \n",
    "\n",
    "Luego de quedarnos con esa columna, generamos un booleano que ponga True cuando los valores sean mayores a 0.5 (el umbral por default que usan los modelos en sklearn) y False cuando sean menores. \n",
    "\n",
    "Finalmente, pasamos ese booleano a valores de 0 ó 1, y chequeamos que obtenemos la misma distribución que con los resultados originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertenencia a partir de las probabilidades\n",
    "pd.Series(y_pred_proba[:,1]>0.5).astype(int).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertenencia calculada a partir del método .predict()\n",
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y si comparamos una serie contra la otra, encontramos que son idénticas ya que en todos los casos obtenemos **True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(pd.Series(y_pred_proba[:,1]>0.5).astype(int)==pd.Series(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos ahora una **distribución de las probabilidades** para inspeccionar un poco más en detalle los resultados que nos provee `.predict_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con la columna que tiene la probabilidad positiva de cada caso\n",
    "y_probs_logit_left = y_pred_proba[:,1]\n",
    "\n",
    "# Generamos un histograma de esa columna\n",
    "plt.hist(y_probs_logit_left, bins=15)\n",
    "plt.xlim(0,1)\n",
    "plt.title('Histograma de probabilidades estimadas')\n",
    "plt.xlabel('Probabilidad estimada de dejar la empresa')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué tenemos aquí?**\n",
    "- La gran mayoría de las probabilidades predichas van de 0.0 a 0.4\n",
    "- Hay un escaso número de probabilidades estimadas mayores a 0.5 (es decir, de empleados que se van)\n",
    "- En consecuencia, la mayor parte de los casos van a ser predichos como $y=0$ es decir, que no se van de la empresa.\n",
    "\n",
    "**¿Y para qué nos sirve conocer esto?**\n",
    "- Dependiendo de nuestro objetivo y de la lógica del negocio, podemos querer modificar este umbral para que, por ejemplo, sea menos riguroso y así identifique como empleados que se van a ir a casos que estén por arriba de 0.4. Esto quizás aumenta la cantidad de **TRUE POSITIVES** (y por lo tanto de nuestro **recall** o **sensibilidad**). Es decir, el clasificador será más **\"sensible\"** a las instancias positivas\n",
    "- Peeeeeeeero..., esto también nos hace cometer más errores de **FALSOS POSITIVOS** ya que quizás vamos a identificar casos como empleados que se van, cuando en realidad se quedan. Lo que va a afectar el **specificity** (y también la **precision**). \n",
    "- Movamos un poco el umbral (aunque no demasiado) para observar cómo cambia la matriz de confusión y ver si se cumple esto que estamos diciendo:\n",
    "\n",
    "<img src=\"img/14_thresholds.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos dos opciones para hacer las transformaciones con distintos thresholds. Podemos utilizar la función `binarize` del módulo **preprocessing** de **sklearn** (vean aquí [la documentación](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import binarize\n",
    "y_pred_logit = binarize(y_pred_proba, 0.3)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra opción, es genear un booleano y pasarlo a int (como hicimos arriba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logit = (y_probs_logit_left > 0.3).astype(int)\n",
    "y_pred_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ahora la **matriz de confusión original** y cómo cambia si usamos el threshold de 0.3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Matrix Original')\n",
    "display(confusion_matrix(y_test,y_pred))\n",
    "print('')\n",
    "print('Nueva Matrix')\n",
    "display(confusion_matrix(y_test,y_pred_logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la **Matrix de confusión** podemos ver cómo aumentaron los **TP** (pasaron de 283 a 823) peeeeero, también nos aumentaron los **FP** ya que pasaron de 305 a 723. \n",
    "\n",
    "Veamos cómo reflejan esto algunas de las métricas de evaluación que vimos antes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sensitivity** o **recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Recall umbral 0.5=', recall_score(y_test, y_pred))\n",
    "print('Recall umbral 0.3=', recall_score(y_test, y_pred_logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specificity**\n",
    "\n",
    "(Dado que sklearn no nos brinda una función para calcular la specificidad, vamos a definir una función que nos permita estimarla rápidamente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos la función que va a tomar como argumentos los valores reales y los predichos\n",
    "def specificy(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    return(specificity)\n",
    "\n",
    "print('Spec umbral 0.5 =', specificy(y_test, y_pred))\n",
    "print('Spec umbral 0.3 =', specificy(y_test, y_pred_logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acc umbral 0.5 =', accuracy_score(y_test, y_pred))\n",
    "print('Acc umbral 0.3 =', accuracy_score(y_test, y_pred_logit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja7\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/kit_de_salida.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <br>  \n",
    "  <div style=\"float:left;width: 85%;\"><label><b>¿Qué podemos aprender de esto?</b></label></div>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "- Se puede **ajustar el umbral** para las predicciones en **clasificadores binarios**.\n",
    "- El ajuste de este umbral **repercute sobre las diferentes medidas de performance**.\n",
    "- Particularmente, **sensitivity** y **specificity** tiene una relación inversa: siempre al mejorar uno, empeorará el otro\n",
    "- En nuestro ejemplo, el **accuracy** no se vio demasiado afectado porque aunque ganamos en **TP**, perdimos en **TN** (ya que aumentó nuestra cantidad de **FP**). Dado que en el **accuracy** lo que importa es la cantidad de **TP**+**TN** sobre el total de casos, es lógico que si aumentamos uno pero bajamos el otro, esta métrica no cambie demasiado\n",
    "- **¿Para qué nos sirve modificar los umbrales?** Ante ciertos problemas o lógicas de negocios, podemos querer cambiar el umbral de decisión de nuestro modelo para mejorar nuestra detección de casos positivos, a pesar de que aumentemos los negativos (Imagínense que estamos desarrollando una prueba muy sencilla para detectar demencias en adultos mayores cuyo objetivo es aplicarla masivamente y, en los casos que sean detectados como positivos, recomendarle a la persona que visite a un neurólogo. Aunque la recomendación va a tener un costo para la persona, nos va a interesar más que la medida sea muy sensible para que no se le \"escapen\" casos positivos, a riesgo de que personas sin problemas estén visitando al neurólogo. En este caso, es mejor generar un costo económico extra pero asegurarse que no están quedando pacientes sin identificar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/para_seguir_pensando.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <br>    \n",
    "  <div style=\"float:left;width: 85%;\"><label><b>¿Se te ocurren otras situaciones en los que se podría modificar el umbral acorde a la lógica del negocio o problema?</b></label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a ver otra medida para evaluar modelos que, en su base, genera distintos modelos a partir de mover los **umbrales de decisión** desde valores muy permisivos a valores muy exigentes. Nos referimos a la **CURVA ROC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_croc\"></a> \n",
    "### 3.2 Introducción Curva ROC y AUC\n",
    "\n",
    "La **Curva ROC** es un plot que nos permite evaluar en un modelo la relación entre dos métricas que vimos anteriormente: **specificity** (que también se calcula como $1-FPR$, tasa de falsos negativos) y **sensitivity** (o también denominada **recall** o $TPR$, tasa de verdaderos positivos). \n",
    "<img src=\"img/15_roc_curve_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "La clave para realizar el plot de la **Curva ROC** es estimar la **specificity** y **sensitivity** de un modelo para distintos **umbrales de decisión** (recuerden lo que vimos en la sección anterior sobre cómo se ve afectado los resultados de un modelo cuando cambiamos este umbral)\n",
    "\n",
    "Antes de probar con nuestros datos, veamos un ejemplo. Asumamos que hemos calculado **specificty** y **sensitivity** para cuatro matrices de confusión distintas que son el resultado de elegir 4 **umbrales de decisión** (de menos exigente a más exigente). Los **umbrales** que elegimos para este ejemplo serían: **1 (umbral 1)**, **0.50 (umbral 2)**, **0.75 (umbral 3)** y **0 (umbral 4)**. Fijensé que el **umbral 1** va a ser el más restrictivo, ya que lo que está indicando es que para que un caso sea considerado positivo su valor de probabilidad tendría que ser mayor que 1 (lo que es imposible, por lo que con ese umbral todos nuestros casos van a ser clasificados como negativos y nuestra **specificity** será máxima). Veamos cómo quedaría nuestra tabla para el modelo imaginario: \n",
    "<img src=\"img/16_curva_roc_2.JPG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Fijensé que la tabla ilustra muy bien cómo a medida que aumentamos el umbral y nos volvemos más flexibles ganamos **sensitivity** pero perdemos **specificity**. \n",
    "\n",
    "Utilizando la información de esta tabla podemos armar el plot de la **Curva ROC**. Para ello, vamos a ubicar cada uno de estos resultados en un gráfico cuyo **eje y** corresponderá a la **sensitivity** de cada umbral, mientras que el **eje x** corresponderá a la **FPR** o (**1-specificity**) de cada umbral (se usa la inversa de **specificity** para que los dos ejes del gráfico aumenten en forma positiva. Y para que se arme la **curva** vamos a unir esos puntos. Veamos cómo queda el plot: \n",
    "<img src=\"img/17_curve_roc_3.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "**¿Cómo interpretamos la Curva ROC?** Veamos ejemplos de cómo es la **Curva ROC** de distintos modelos para entender cuándo estamos ante un modelo que tiende a ser bueno y cuando random: \n",
    "<img src=\"img/18_curva_roc_4.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "Un ejemplo de cómo se vería un modelo **perfecto** (fijense que tiene el máximo de **sensitivitiy** y el máximo de **specificity**): \n",
    "<img src=\"img/19_curva_roc_.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "Los modelos que son **\"buenos\"** suelen estar entre la línea del **random** y del **perfect**. \n",
    "\n",
    "La **Curva ROC** nos permite visualmente comparar el desempeño de dos modelos en dos medidas que son claves como **sensitivity** y **specificity**. Aquel modelo que esté más cerca de la curva **perfecta** va a ser mejor que aquellos que estén más cerca de la línea de **random** (en el siguiente gráfico A > B): \n",
    "<img src=\"img/20_curva_roc_6.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "\n",
    "Por último, la **Curva ROC** nos permite estima el **Area bajo la curva (AUC por sus siglas en inglés, Area under the curve)**. El **AUC** intenta resumir en una sóla métrica la relación que muestra el gráfico entre **sensitivity** y **specificity** y así poder comparar modelos utilizando esa métrica: \n",
    "<img src=\"img/21_curva_roc_7.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "**CUIDADO** con el **AUC** porque dos modelos pueden tener un mismo **AUC** pero ser distintos si uno visualiza su **Curva ROC**:\n",
    "<img src=\"img/22_curva_roc_8.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja10\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/ponete_a_prueba.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <br>\n",
    "  <div style=\"float:left;width: 85%;\"><label><b>¿Entonces... </b></label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecutá esta celda...\n",
    "test_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_imp_croc\"></a> \n",
    "### 3.3 Implementación Curva ROC y AUC\n",
    "\n",
    "Veamos ahora cómo podemos estimar la **Curva ROC** y el **AUC** en nuestro modelo para detectar los empleados que se van a ir de la empresa. \n",
    "\n",
    "**¿Cómo plotear la Curva ROC?** Vamos a utilizar la función [`.roc_curve()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) del módulo `metrics` de `sklearn`. Esta función nos va a calcular para distintos thesholds, el **FPR** (**1-specificity**) y el **TPR** (**sensitivity o recall**). Con estos valores vamos a poder hacer el gráfico de la curva:\n",
    "- **Documentación:** [`.roc_curve()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
    "- **Un ejemplo de su uso:** [caso ejemplo](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics)\n",
    "- En `.roc_curve()` el primer argumento corresponde a los **valores observados** y el segundo a puede ser la **probabilidad de la clase positiva**\n",
    "- Como **output** nos va a devolver (respetando el orden): **FPR**, **TPR**, **THRESHOLDS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos `.roc_curve()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr_log,tpr_log,thr_log = roc_curve(y_test, y_pred_proba[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos los valores en un objeto dataframe y graficamos la curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict(fpr=fpr_log, tpr=tpr_log, thr = thr_log))\n",
    "\n",
    "plt.axis([0, 1.01, 0, 1.01])\n",
    "plt.xlabel('1 - Specificty')\n",
    "plt.ylabel('TPR / Sensitivity')\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(df['fpr'],df['tpr'])\n",
    "plt.plot(np.arange(0,1, step =0.01), np.arange(0,1, step =0.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo estimamos la AUC?** Vamos a utilizar la función [`.auc()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html) del módulo `metrics` de `sklearn`. Esta función nos va a calcular el **AUC** para un modelo a partir de los valores de **TPR** y **FPR**:\n",
    "- **Documentación:** [`.auc()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html)\n",
    "- **Un ejemplo de su uso:** [caso ejemplo](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics)\n",
    "- En `.auc()` el primer argumento corresponde a los valores de **FPR** y el segundo a **TPR**\n",
    "- Como **output** nos va a devolver el **AUC**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos `.auc()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "print('AUC=', auc(fpr_log, tpr_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar de ver la aplicabilidad de la `.roc_curve()` y `.auc()`, comparamos el modelo que teníamos con otro (clasificador Naïve Bayes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nbc = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc.fit(X_train, y_train)\n",
    "y_preds_nb = nbc.predict(X_test)\n",
    "y_probs_nb = nbc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_nb,tpr_nb,thr_nb = roc_curve(y_test, y_probs_nb[:,1])\n",
    "fpr_log,tpr_log,thr_log = roc_curve(y_test, y_pred_proba[:,1])\n",
    "\n",
    "plt.axis([0, 1.01, 0, 1.01])\n",
    "plt.xlabel('1 - Specificty')\n",
    "plt.ylabel('TPR / Sensitivity')\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr_nb,tpr_nb)\n",
    "plt.plot(fpr_log,tpr_log)\n",
    "plt.plot(np.arange(0,1, step =0.01), np.arange(0,1, step =0.01))\n",
    "plt.legend(['NB','Logit'])\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC-NB=', auc(fpr_nb, tpr_nb))\n",
    "print('AUC-Logit=', auc(fpr_log, tpr_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja9\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/para_seguir_pensando.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <br>    \n",
    "  <div style=\"float:left;width: 85%;\"><label><b>¿Cuál modelo es mejor?</b></label></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.\"></a> \n",
    "## 4. Comentarios finales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/23_conclusion.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"caja7\" style=\"float:left;width: 100%;\">\n",
    "  <div style=\"float:left;width: 15%;\"><img src=\"../../../common/icons/en_resumen.png\" style=\"align:left\"/> </div>\n",
    "  <br>\n",
    "  <br>  \n",
    "  <div style=\"float:left;width: 85%;\"><label><b>En conclusión...</b></label></div>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "- En los **problemas de CLASIFICACIÓN** el objetivo es predecir la pertenencia o la probabilidad de pertenencia de un caso a una clase.\n",
    "\n",
    "\n",
    "- Para estos problemas vamos a contar herramientas y métricas para **evaluar el desempeño del modelo** tales como: la **matriz de confusión**, el **accuracy**, **sensitivity o recall**, **specificity**, **precision**, **f1-score**, **curva ROC** y **AUC**.\n",
    "\n",
    "\n",
    "- Las herramientas y métricas de evaluación nos brindan información de qué tan bien nuestro modelo está identificando la pertenencia de un caso a una clase. Cada una de las distintas herramientas y métricas nos permite **hacer foco en distintos aspectos** de su desempeño, y evaluar cuáles son los **tipos de errores** que más está cometiendo.\n",
    "\n",
    "\n",
    "- Además, nos van a permitir **comparar distintos modelos** que entrenemos y tomar decisiones  respecto de cuál es el óptimo para usar (siempre teniendo en cuenta el contexto del problema y la lógica de negocio que tenga detrás)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='#tabla_contenidos'>Volver a TOC</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
